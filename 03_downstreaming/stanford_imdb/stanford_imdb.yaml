model_path: "google/electra-small-discriminator" #"/data/language_models/pretrained_models/mlm_00/mlm_model"
num_labels: 2
tokenizer_path: "google/electra-small-discriminator" #"/data/language_models/tokenizers/finlm-wordpiece-tokenizer.json"
max_sequence_length: 128
dataset_name: "stanford_imbd"
text_column: "text"
dataset_columns: ["input_ids", "attention_mask", "label"]
training_data_fraction: 0.5    
batch_size: 32
max_sequences: 32
early_stopping_patience: 3
n_splits: 4
n_epochs:
  name: "n_epochs"
  dtype: int
  low: 1
  high: 5
  default: 3
learning_rate:
  name: "learning_rate"
  dtype: float
  low:  0.0001
  high: 0.0002
  default: 0.00015
classifier_dropout: 
  name: "classifier_dropout"
  dtype: float
  low: 0.1
  high: 0.9
  default: 0.5
warmup_step_fraction:
  name: "warmup_step_fraction"
  dtype: float
  low: 0.01
  high: 0.10
  default: 0.05
use_gradient_clipping:
  name: "use_gradient_clipping"
  dtype: bool
  low: None
  high: None
  default: False
save_path: "/data/language_models/pretrained_models_downstreaming/stanford_imdb/electra_small_discriminator_document_predictions"