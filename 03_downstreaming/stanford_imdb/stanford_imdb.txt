2024-08-12 18:37:17,043 : INFO : Starting to tokenize the sequences.
2024-08-12 18:37:17,054 : INFO : Tokenization is finished.
2024-08-12 18:37:17,080 : INFO : Occurence of labels for training data: (array([0, 1]), array([12500, 12500]))
2024-08-12 18:37:17,084 : INFO : Occurence of labels for test data: (array([0, 1]), array([12500, 12500]))
[I 2024-08-12 18:37:17,084] A new study created in memory with name: no-name-93465af4-f7d0-43c8-9c83-9bf5a03151d0
2024-08-12 18:37:17,084 : INFO : ----------------------------------------------------------------------------------------------------
2024-08-12 18:37:17,084 : INFO : Starting training for split 1
2024-08-12 18:37:17,091 : INFO : Occurence of labels for training split: (array([0, 1]), array([9405, 9345]))
2024-08-12 18:37:17,093 : INFO : Occurence of labels for validation split: (array([0, 1]), array([3095, 3155]))
Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-08-12 18:37:18,170 : INFO : Starting epoch 1
2024-08-12 18:37:19,277 : INFO : Current training batch loss: 0.7006 in epoch 1
2024-08-12 18:37:36,440 : INFO : Current training batch loss: 0.9087 in epoch 1
2024-08-12 18:37:53,599 : INFO : Current training batch loss: 0.5554 in epoch 1
2024-08-12 18:38:10,794 : INFO : Current training batch loss: 0.6533 in epoch 1
2024-08-12 18:38:27,924 : INFO : Current training batch loss: 0.7094 in epoch 1
2024-08-12 18:38:45,001 : INFO : Current training batch loss: 0.6917 in epoch 1
2024-08-12 18:39:02,078 : INFO : Current training batch loss: 0.6928 in epoch 1
