{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from finlm.models import ElectraDocumentClassification\n",
    "from finlm.dataset import FinetuningDocumentDataset\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "finetuning_model_path = \"/data/language_models/pretrained_models_downstreaming/stanford_imdb/electra_small_discriminator_document_predictions/finetuning_config.json\"\n",
    "with open(finetuning_model_path, \"r\") as file:\n",
    "    finetuning_config = json.load(file)\n",
    "\n",
    "model_loader = lambda model_path, num_labels, classifier_dropout: ElectraDocumentClassification.from_pretrained(model_path, num_labels = num_labels, classifier_dropout = classifier_dropout, num_sequence_attention_heads = 1) \n",
    "\n",
    "if not(torch.cuda.is_available()):\n",
    "    print(\"GPU seems to be unavailable.\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "# Split the dataset into training and test data\n",
    "training_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# datasets must be shuffled, because they are sorted by label\n",
    "training_data = training_data.shuffle(42)\n",
    "test_data = test_data.shuffle(42)\n",
    "\n",
    "training_documents, training_labels = [], []\n",
    "for sample in training_data:\n",
    "    training_documents.append(sample[\"text\"])\n",
    "    training_labels.append(sample[\"label\"])\n",
    "\n",
    "test_documents, test_labels = [], []\n",
    "for sample in test_data:\n",
    "    test_documents.append(sample[\"text\"])\n",
    "    test_labels.append(sample[\"label\"])\n",
    "\n",
    "training_documents = [re.split(r'(?<=[.!?]) +', doc) for doc in training_documents]\n",
    "test_documents = [re.split(r'(?<=[.!?]) +', doc) for doc in test_documents]\n",
    "\n",
    "training_dataset = FinetuningDocumentDataset(documents = training_documents, labels = training_labels, tokenizer_path = finetuning_config[\"tokenizer_path\"], sequence_length = finetuning_config[\"max_sequence_length\"])\n",
    "test_dataset = FinetuningDocumentDataset(documents = test_documents, labels = test_labels, tokenizer_path = finetuning_config[\"tokenizer_path\"], sequence_length = finetuning_config[\"max_sequence_length\"])\n",
    "\n",
    "model = model_loader(\n",
    "    os.path.join(finetuning_config[\"save_path\"], \"finetuned_model\"),\n",
    "    finetuning_config[\"num_labels\"],\n",
    "    0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from finlm.dataset import collate_fn_fixed_sequences\n",
    "\n",
    "collate_fn = lambda x: collate_fn_fixed_sequences(x, max_sequences = finetuning_config[\"max_sequences\"])\n",
    "training_data = DataLoader(training_dataset, 1, shuffle = False, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(training_data):\n",
    "    if i == 1:\n",
    "        break\n",
    "\n",
    "inputs, attention_mask, labels, sequence_mask = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"label\"].to(device), batch[\"sequence_mask\"].to(device)\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = model(input_ids = inputs, attention_mask = attention_mask, sequence_mask = sequence_mask, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.867501 , 2.8211923, 4.6331573, 3.7683778, 4.091405 , 1.4792961,\n",
       "       2.217933 , 5.3355713, 2.7855694, 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       ], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_aggregate = model_output.attentions[0, 0, :, :].sum(dim = 0).cpu().numpy()\n",
    "attention_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.3355713, 4.867501 , 4.6331573, 4.091405 , 3.7683778, 2.8211923,\n",
       "       2.7855694, 2.217933 , 1.4792961, 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       ], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sorted_index = np.flip(attention_aggregate.argsort())\n",
    "attention_aggregate[sorted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This movie is a great.',\n",
       " 'The plot is very true to the book which is a classic written by Mark Twain.',\n",
       " 'The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra\\'s song High Hopes, it is fun and inspirational.',\n",
       " 'The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore.',\n",
       " 'OVerall a great family movie or even a great Date movie.',\n",
       " 'This is a movie you can watch over and over again.',\n",
       " 'The princess played by Rhonda Fleming is gorgeous.',\n",
       " 'I love this movie!!',\n",
       " 'If you liked Danny Kaye in the Court Jester then you will definitely like this movie.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_documents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love this movie!!',\n",
       " 'This movie is a great.',\n",
       " 'The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra\\'s song High Hopes, it is fun and inspirational.',\n",
       " 'OVerall a great family movie or even a great Date movie.',\n",
       " 'The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore.',\n",
       " 'The plot is very true to the book which is a classic written by Mark Twain.',\n",
       " 'If you liked Danny Kaye in the Court Jester then you will definitely like this movie.',\n",
       " 'The princess played by Rhonda Fleming is gorgeous.',\n",
       " 'This is a movie you can watch over and over again.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[training_documents[i][idx] for idx in sorted_index[:len(training_documents[i])]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
