{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/language_models/pretrained_models_downstreaming/stanford_imdb/electra_small_discriminator_document_predictions/finetuned_model'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.join(finetuning_config[\"save_path\"], \"finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from finlm.models import ElectraDocumentClassification\n",
    "from finlm.dataset import FinetuningDocumentDataset\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "finetuning_model_path = \"/data/language_models/pretrained_models_downstreaming/stanford_imdb/electra_small_discriminator_document_predictions/finetuning_config.json\"\n",
    "with open(finetuning_model_path, \"r\") as file:\n",
    "    finetuning_config = json.load(file)\n",
    "\n",
    "model_loader = lambda model_path, num_labels, classifier_dropout: ElectraDocumentClassification.from_pretrained(model_path, num_labels = num_labels, classifier_dropout = classifier_dropout, num_sequence_attention_heads = 1) \n",
    "\n",
    "if not(torch.cuda.is_available()):\n",
    "    print(\"GPU seems to be unavailable.\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "# Split the dataset into training and test data\n",
    "training_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# datasets must be shuffled, because they are sorted by label\n",
    "training_data = training_data.shuffle(42)\n",
    "test_data = test_data.shuffle(42)\n",
    "\n",
    "training_documents, training_labels = [], []\n",
    "for sample in training_data:\n",
    "    training_documents.append(sample[\"text\"])\n",
    "    training_labels.append(sample[\"label\"])\n",
    "\n",
    "test_documents, test_labels = [], []\n",
    "for sample in test_data:\n",
    "    test_documents.append(sample[\"text\"])\n",
    "    test_labels.append(sample[\"label\"])\n",
    "\n",
    "training_documents = [re.split(r'(?<=[.!?]) +', doc) for doc in training_documents]\n",
    "test_documents = [re.split(r'(?<=[.!?]) +', doc) for doc in test_documents]\n",
    "\n",
    "training_dataset = FinetuningDocumentDataset(documents = training_documents, labels = training_labels, tokenizer_path = finetuning_config[\"tokenizer_path\"], sequence_length = finetuning_config[\"max_sequence_length\"])\n",
    "test_dataset = FinetuningDocumentDataset(documents = test_documents, labels = test_labels, tokenizer_path = finetuning_config[\"tokenizer_path\"], sequence_length = finetuning_config[\"max_sequence_length\"])\n",
    "\n",
    "model = model_loader(\n",
    "    os.path.join(finetuning_config[\"save_path\"], \"finetuned_model\"),\n",
    "    finetuning_config[\"num_labels\"],\n",
    "    0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from finlm.dataset import collate_fn_fixed_sequences\n",
    "\n",
    "collate_fn = lambda x: collate_fn_fixed_sequences(x, max_sequences = finetuning_config[\"max_sequences\"])\n",
    "training_data = DataLoader(training_dataset, 1, shuffle = False, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(training_data):\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "inputs, attention_mask, labels, sequence_mask = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"label\"].to(device), batch[\"sequence_mask\"].to(device)\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = model(input_ids = inputs, attention_mask = attention_mask, sequence_mask = sequence_mask, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.17128  , 4.6234665, 7.7983274, 2.3347297, 1.9695232, 1.3553371,\n",
       "       1.2834724, 1.3471363, 1.2095077, 1.414024 , 3.8016944, 1.6915021,\n",
       "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       ], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_aggregate = model_output.attentions[0, 0, :, :].sum(dim = 0).cpu().numpy()\n",
    "attention_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.7983274, 4.6234665, 3.8016944, 3.17128  , 2.3347297, 1.9695232,\n",
       "       1.6915021, 1.414024 , 1.3553371, 1.3471363, 1.2834724, 1.2095077,\n",
       "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "       0.       , 0.       ], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sorted_index = np.flip(attention_aggregate.argsort())\n",
    "attention_aggregate[sorted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes.',\n",
       " 'Profiler looks crispy, Fortier looks classic.',\n",
       " 'Profiler plots are quite simple.',\n",
       " \"Fortier's plot are far more complicated...\",\n",
       " 'Fortier looks more like Prime Suspect, if we have to spot similarities...',\n",
       " 'The main character is weak and weirdo, but have \"clairvoyance\".',\n",
       " 'People like to compare, to judge, to evaluate.',\n",
       " 'How about just enjoying?',\n",
       " 'Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!).',\n",
       " \"Maybe it's the language, or the spirit, but I think this series is more English than American.\",\n",
       " 'By the way, the actors are really good and funny.',\n",
       " 'The acting is not superficial at all...']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_documents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Profiler plots are quite simple.',\n",
       " 'Profiler looks crispy, Fortier looks classic.',\n",
       " 'By the way, the actors are really good and funny.',\n",
       " 'There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes.',\n",
       " \"Fortier's plot are far more complicated...\",\n",
       " 'Fortier looks more like Prime Suspect, if we have to spot similarities...',\n",
       " 'The acting is not superficial at all...',\n",
       " \"Maybe it's the language, or the spirit, but I think this series is more English than American.\",\n",
       " 'The main character is weak and weirdo, but have \"clairvoyance\".',\n",
       " 'How about just enjoying?',\n",
       " 'People like to compare, to judge, to evaluate.',\n",
       " 'Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!).']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[training_documents[i][idx] for idx in sorted_index[:len(training_documents[i])]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
