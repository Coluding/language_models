dataset_config:
  tokenizer_path: "/home/ralf/language_models/00_tokenizer/finlm-wordpiece-tokenizer.json"
  max_sequence_length: 256
  db_name: "/data/finlm_sequences/finlm_chunks.sqlite"
  batch_size: 128
  database_retrieval:
    form_tenk:
      limit: 1000000
      offset: 0
    form_eightk:
      limit: 1000000
      offset: 0
    earning_calls:
      limit: 1000000
      offset: 0
    tr_news: 
      limit: 1000000
      offset: 0 
model_config:
  embedding_size: 128
  hidden_size: 256
  num_hidden_layers: 12
  num_attention_heads: 4
  intermediate_size: 1024
  generator_size: 0.25
  generator_layer_size: 1.0
optimization_config:
  learning_rate: 0.0001
  n_epochs: 5
  lr_scheduler_warm_up_steps: 10000
  mlm_probability: 0.15
  use_gradient_clipping: True
  discriminator_weight: 50
  discriminator_sampling: "gumbel_softmax"