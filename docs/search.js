window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "finlm", "modulename": "finlm", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "finlm.callbacks", "modulename": "finlm.callbacks", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "finlm.callbacks.EarlyStopping", "modulename": "finlm.callbacks", "qualname": "EarlyStopping", "kind": "class", "doc": "<p>Early stopping utility to stop training when a monitored metric has stopped improving.</p>\n\n<p>This class monitors a validation metric during training and stops training when the metric \nfails to improve after a given number of epochs (patience). It can operate in both 'min' \nmode (for metrics that should decrease) and 'max' mode (for metrics that should increase).\nWhen an improvement is detected, the current model is saved to a specified directory.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>patience : int\n    Number of epochs to wait after the last improvement in the monitored metric.\nverbose : bool\n    If True, logs information about improvements and early stopping.\ndelta : float\n    Minimum change in the monitored metric to qualify as an improvement.\nmode : str\n    One of {'min', 'max'}. Specifies whether to stop training when the metric stops decreasing ('min') or increasing ('max').\nsave_path : str\n    Directory where model checkpoints are saved.\ncounter : int\n    Number of epochs without improvement.\nbest_score : float or None\n    Best score observed for the monitored metric.\nearly_stop : bool\n    Flag indicating whether early stopping has been triggered.\nlogger : logging.Logger\n    Logger instance for logging messages related to early stopping.\nval_metric_min : float\n    Minimum value of the validation metric observed (used in 'min' mode).\nval_metric_max : float\n    Maximum value of the validation metric observed (used in 'max' mode).</p>\n"}, {"fullname": "finlm.callbacks.EarlyStopping.__init__", "modulename": "finlm.callbacks", "qualname": "EarlyStopping.__init__", "kind": "function", "doc": "<p>Initializes the EarlyStopping instance with the specified configuration.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>patience : int, optional\n    How long to wait after the last time the validation metric improved (default is 5).\nverbose : bool, optional\n    If True, prints a message for each validation metric improvement (default is False).\ndelta : float, optional\n    Minimum change in the monitored quantity to qualify as an improvement (default is 0).\nmode : str, optional\n    One of {'min', 'max'}. Specifies whether to stop training when the metric stops decreasing ('min') or increasing ('max') (default is 'min').\nsave_path : str, optional\n    Directory where model checkpoints will be saved (default is 'checkpoints').</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">patience</span><span class=\"o\">=</span><span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">delta</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;min&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;checkpoints&#39;</span></span>)</span>"}, {"fullname": "finlm.callbacks.EarlyStopping.patience", "modulename": "finlm.callbacks", "qualname": "EarlyStopping.patience", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.callbacks.EarlyStopping.verbose", "modulename": "finlm.callbacks", "qualname": "EarlyStopping.verbose", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.callbacks.EarlyStopping.delta", "modulename": "finlm.callbacks", "qualname": "EarlyStopping.delta", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.callbacks.EarlyStopping.mode", "modulename": "finlm.callbacks", "qualname": "EarlyStopping.mode", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.callbacks.EarlyStopping.counter", "modulename": "finlm.callbacks", "qualname": "EarlyStopping.counter", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.callbacks.EarlyStopping.best_score", "modulename": "finlm.callbacks", "qualname": "EarlyStopping.best_score", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.callbacks.EarlyStopping.early_stop", "modulename": "finlm.callbacks", "qualname": "EarlyStopping.early_stop", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.callbacks.EarlyStopping.save_path", "modulename": "finlm.callbacks", "qualname": "EarlyStopping.save_path", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.callbacks.EarlyStopping.logger", "modulename": "finlm.callbacks", "qualname": "EarlyStopping.logger", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.callbacks.EarlyStopping.save_checkpoint", "modulename": "finlm.callbacks", "qualname": "EarlyStopping.save_checkpoint", "kind": "function", "doc": "<p>Saves the model when the validation metric improves.</p>\n\n<p>This method saves the current state of the model if the validation metric shows improvement.\nIt also logs the improvement if <code>verbose</code> is set to True.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>val_metric : float\n    The current value of the validation metric that triggered the checkpoint save.\nmodel : torch.nn.Module\n    The model to be saved.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">val_metric</span>, </span><span class=\"param\"><span class=\"n\">model</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.chunking", "modulename": "finlm.chunking", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "finlm.chunking.Chunker", "modulename": "finlm.chunking", "qualname": "Chunker", "kind": "class", "doc": "<p>A base class for chunking text documents from a database.</p>\n\n<p>This class provides methods for counting documents in a database table and chunking text documents\ninto approximately equal text chunks based on the number of words.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>db_in : str\n    The path to the input SQLite database.\nsheet_in : str\n    The name of the table in the database containing the text documents.\nlimit : int, optional\n    The maximum number of documents to process (default is None).\noffset : int, optional\n    The starting point from which to begin processing documents (default is None).\nn_documents : int\n    The total number of documents in the specified table.\nlogger : logging.Logger\n    Logger instance for logging messages related to chunking operations.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>count_documents() -> int\n    Counts the number of documents in the specified table.\nchunk_to_database(db_out: str, sheet_out: str, chunk_size_in_words: int, ignore_first_sentences: int, ignore_last_sentences: int) -> None\n    Chunks the documents into equal text chunks and saves them to a new database.\nsplit_text_into_chunks_by_words(text: str, max_words_per_chunk: int, ignore_first_sentences: int = None, ignore_last_sentences: int = None) -> list[str]\n    Splits the input text into chunks based on the number of words, keeping sentences together.</p>\n"}, {"fullname": "finlm.chunking.Chunker.__init__", "modulename": "finlm.chunking", "qualname": "Chunker.__init__", "kind": "function", "doc": "<p>Initializes the Chunker with database and table information.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>db_in : str\n    The path to the input SQLite database.\nsheet_in : str\n    The name of the table in the database containing the text documents.\nlimit : int, optional\n    The maximum number of documents to process (default is None).\noffset : int, optional\n    The starting point from which to begin processing documents (default is None).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">db_in</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">sheet_in</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">limit</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">offset</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "finlm.chunking.Chunker.db_in", "modulename": "finlm.chunking", "qualname": "Chunker.db_in", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.chunking.Chunker.sheet_in", "modulename": "finlm.chunking", "qualname": "Chunker.sheet_in", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.chunking.Chunker.limit", "modulename": "finlm.chunking", "qualname": "Chunker.limit", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.chunking.Chunker.offset", "modulename": "finlm.chunking", "qualname": "Chunker.offset", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.chunking.Chunker.n_documents", "modulename": "finlm.chunking", "qualname": "Chunker.n_documents", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.chunking.Chunker.logger", "modulename": "finlm.chunking", "qualname": "Chunker.logger", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.chunking.Chunker.count_documents", "modulename": "finlm.chunking", "qualname": "Chunker.count_documents", "kind": "function", "doc": "<p>Counts the number of documents in the specified table.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>int\n    The total number of documents in the specified table.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">int</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.chunking.Chunker.chunk_to_database", "modulename": "finlm.chunking", "qualname": "Chunker.chunk_to_database", "kind": "function", "doc": "<p>Chunks the documents into equal text chunks and saves them to a new database.</p>\n\n<p>This method processes each document in the specified table, splitting it into chunks of \napproximately equal size (based on word count) and saving the resulting chunks to a new table.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>db_out : str\n    The path to the output SQLite database where the chunked sequences will be saved.\nsheet_out : str\n    The name of the table where the chunked sequences will be saved.\nchunk_size_in_words : int\n    The desired number of words per chunk.\nignore_first_sentences : int\n    The number of initial sentences to exclude from chunking.\nignore_last_sentences : int\n    The number of final sentences to exclude from chunking.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">db_out</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">sheet_out</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">chunk_size_in_words</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_first_sentences</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_last_sentences</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.chunking.Chunker.split_text_into_chunks_by_words", "modulename": "finlm.chunking", "qualname": "Chunker.split_text_into_chunks_by_words", "kind": "function", "doc": "<p>Splits the input text into chunks based on the number of words, keeping sentences together.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>text : str\n    The input text to be split into chunks.\nmax_words_per_chunk : int\n    The maximum number of words allowed in each chunk.\nignore_first_sentences : int, optional\n    The number of initial sentences to exclude from chunking (default is None).\nignore_last_sentences : int, optional\n    The number of final sentences to exclude from chunking (default is None).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>list[str]\n    A list of text chunks, each containing up to <code>max_words_per_chunk</code> words.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">max_words_per_chunk</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_first_sentences</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_last_sentences</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.chunking.Form10KChunker", "modulename": "finlm.chunking", "qualname": "Form10KChunker", "kind": "class", "doc": "<p>A class for chunking 10-K form filings from a database.</p>\n\n<p>This class inherits from <code>Chunker</code> and is specifically designed to process 10-K forms, \nchunking them into approximately equal sequences and exporting the chunks to a new database.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>__iter__()\n    A generator that yields the text of 10-K forms from the database.</p>\n", "bases": "Chunker"}, {"fullname": "finlm.chunking.Form10KChunker.__init__", "modulename": "finlm.chunking", "qualname": "Form10KChunker.__init__", "kind": "function", "doc": "<p>Initializes the Form10KChunker with database and table information.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>db_in : str\n    The path to the input SQLite database.\nsheet_in : str\n    The name of the table in the database containing the 10-K forms.\nlimit : int, optional\n    The maximum number of documents to process (default is None).\noffset : int, optional\n    The starting point from which to begin processing documents (default is None).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">db_in</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">sheet_in</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">limit</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">offset</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "finlm.chunking.Form8KChunker", "modulename": "finlm.chunking", "qualname": "Form8KChunker", "kind": "class", "doc": "<p>A class for chunking 8-K form filings from a database.</p>\n\n<p>This class inherits from <code>Chunker</code> and is specifically designed to process 8-K forms, \nfocusing on press release statements (by Exhibit identifier) and filtering out documents with excessive punctuation or numbers.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>__iter__()\n    A generator that yields the text of 8-K forms from the database.\ncount_numbers_and_punctuation(s: str) -> float\n    Calculates the fraction of characters in a string that are digits or punctuation.</p>\n", "bases": "Chunker"}, {"fullname": "finlm.chunking.Form8KChunker.__init__", "modulename": "finlm.chunking", "qualname": "Form8KChunker.__init__", "kind": "function", "doc": "<p>Initializes the Form8KChunker with database and table information.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>db_in : str\n    The path to the input SQLite database.\nsheet_in : str\n    The name of the table in the database containing the 8-K forms.\nlimit : int, optional\n    The maximum number of documents to process (default is None).\noffset : int, optional\n    The starting point from which to begin processing documents (default is None).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">db_in</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">sheet_in</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">limit</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">offset</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "finlm.chunking.Form8KChunker.count_numbers_and_punctuation", "modulename": "finlm.chunking", "qualname": "Form8KChunker.count_numbers_and_punctuation", "kind": "function", "doc": "<p>Calculates the fraction of characters in a string that are digits or punctuation.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>s : str\n    The input string to analyze.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>float\n    The fraction of characters in the string that are digits or punctuation.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">s</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.chunking.EarningCallChunker", "modulename": "finlm.chunking", "qualname": "EarningCallChunker", "kind": "class", "doc": "<p>A class for chunking earnings call transcripts from a database.</p>\n\n<p>This class inherits from <code>Chunker</code> and is specifically designed to process earnings call transcripts, \nfiltering out participant names and short sentences, and then exporting the cleaned text to a new database.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>__iter__()\n    A generator that yields the cleaned text of earnings call transcripts from the database.</p>\n", "bases": "Chunker"}, {"fullname": "finlm.chunking.EarningCallChunker.__init__", "modulename": "finlm.chunking", "qualname": "EarningCallChunker.__init__", "kind": "function", "doc": "<p>Initializes the EarningCallChunker with database and table information.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>db_in : str\n    The path to the input SQLite database.\nsheet_in : str\n    The name of the table in the database containing the earnings call transcripts.\nlimit : int, optional\n    The maximum number of documents to process (default is None).\noffset : int, optional\n    The starting point from which to begin processing documents (default is None).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">db_in</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">sheet_in</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">limit</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">offset</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "finlm.chunking.TRNewsChunker", "modulename": "finlm.chunking", "qualname": "TRNewsChunker", "kind": "class", "doc": "<p>A class for chunking Thomson Reuters news articles from a database.</p>\n\n<p>This class inherits from <code>Chunker</code> and is specifically designed to process news articles, \nsplitting them into chunks and exporting the chunks to a new database.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>__iter__()\n    A generator that yields the text of news articles from the database.</p>\n", "bases": "Chunker"}, {"fullname": "finlm.chunking.TRNewsChunker.__init__", "modulename": "finlm.chunking", "qualname": "TRNewsChunker.__init__", "kind": "function", "doc": "<p>Initializes the TRNewsChunker with database and table information.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>db_in : str\n    The path to the input SQLite database.\nsheet_in : str\n    The name of the table in the database containing the news articles.\nlimit : int, optional\n    The maximum number of documents to process (default is None).\noffset : int, optional\n    The starting point from which to begin processing documents (default is None).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">db_in</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">sheet_in</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">limit</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">offset</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "finlm.chunking.rename_table", "modulename": "finlm.chunking", "qualname": "rename_table", "kind": "function", "doc": "<p>Renames a table in a SQLite database, with retries in case the database is locked.</p>\n\n<p>This function attempts to rename a table in the specified SQLite database. If the database is locked,\nit will retry the operation for a specified number of times with a delay between attempts.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>database : str\n    The path to the SQLite database file.\nold_table_name : str\n    The current name of the table to be renamed.\nnew_table_name : str\n    The new name for the table.\nretries : int, optional\n    The number of times to retry the operation if the database is locked (default is 5).\ndelay : int, optional\n    The number of seconds to wait between retry attempts (default is 1).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>bool\n    Returns True if the table was renamed successfully; otherwise, prints a failure message.</p>\n\n<h2 id=\"raises\">Raises</h2>\n\n<p>sqlite3.OperationalError\n    If an unexpected database operation error occurs other than a locked database.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">database</span>, </span><span class=\"param\"><span class=\"n\">old_table_name</span>, </span><span class=\"param\"><span class=\"n\">new_table_name</span>, </span><span class=\"param\"><span class=\"n\">retries</span><span class=\"o\">=</span><span class=\"mi\">5</span>, </span><span class=\"param\"><span class=\"n\">delay</span><span class=\"o\">=</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.chunking.shuffle_and_create_new_table", "modulename": "finlm.chunking", "qualname": "shuffle_and_create_new_table", "kind": "function", "doc": "<p>Creates a new table with shuffled data from an existing table in a SQLite database.</p>\n\n<p>This function creates a new table in the specified SQLite database with the same schema as an existing table.\nThe new table is populated with the data from the original table, but the rows are shuffled randomly.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>database : str\n    The path to the SQLite database file.\noriginal_table : str\n    The name of the original table from which data is to be copied and shuffled.\nshuffled_table : str\n    The name of the new table to create with shuffled data.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>None</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">database</span>, </span><span class=\"param\"><span class=\"n\">original_table</span>, </span><span class=\"param\"><span class=\"n\">shuffled_table</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.config", "modulename": "finlm.config", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "finlm.config.DatasetConfig", "modulename": "finlm.config", "qualname": "DatasetConfig", "kind": "class", "doc": "<p>Configuration class for the dataset used in the FinLM model.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>tokenizer_path : str\n    Path to the tokenizer to be used for text preprocessing.\nmax_sequence_length : int\n    Maximum length of input sequences after tokenization.\ndb_name : str\n    Name of the database where the dataset is stored.\nbatch_size : int\n    Number of sequences to include in each batch.\ndatabase_retrieval : dict[str, dict[str, int]]\n    Dictionary specifying retrieval parameters from the database, including limits and offsets.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>from_dict(data: Dict[str, Any]) -> 'DatasetConfig'\n    Creates an instance of DatasetConfig from a dictionary.</p>\n"}, {"fullname": "finlm.config.DatasetConfig.__init__", "modulename": "finlm.config", "qualname": "DatasetConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tokenizer_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">max_sequence_length</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">db_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">database_retrieval</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span></span>)</span>"}, {"fullname": "finlm.config.DatasetConfig.tokenizer_path", "modulename": "finlm.config", "qualname": "DatasetConfig.tokenizer_path", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "finlm.config.DatasetConfig.max_sequence_length", "modulename": "finlm.config", "qualname": "DatasetConfig.max_sequence_length", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "finlm.config.DatasetConfig.db_name", "modulename": "finlm.config", "qualname": "DatasetConfig.db_name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "finlm.config.DatasetConfig.batch_size", "modulename": "finlm.config", "qualname": "DatasetConfig.batch_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "finlm.config.DatasetConfig.database_retrieval", "modulename": "finlm.config", "qualname": "DatasetConfig.database_retrieval", "kind": "variable", "doc": "<p></p>\n", "annotation": ": dict[str, dict[str, int]]"}, {"fullname": "finlm.config.DatasetConfig.from_dict", "modulename": "finlm.config", "qualname": "DatasetConfig.from_dict", "kind": "function", "doc": "<p>Creates an instance of DatasetConfig from a dictionary.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data : Dict[str, Any]\n    A dictionary containing the configuration parameters.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>DatasetConfig\n    An instance of DatasetConfig initialized with the provided data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">finlm</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">DatasetConfig</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.config.ModelConfig", "modulename": "finlm.config", "qualname": "ModelConfig", "kind": "class", "doc": "<p>Configuration class for the FinLM model architecture.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>embedding_size : int, optional\n    Size of the embedding layer (default is 128).\nhidden_size : int, optional\n    Size of the hidden layers (default is 256).\nnum_hidden_layers : int, optional\n    Number of hidden layers in the model (default is 12).\nnum_attention_heads : int, optional\n    Number of attention heads in each attention layer (default is 4).\nintermediate_size : int, optional\n    Size of the intermediate layer in the transformer (default is 1024).\ngenerator_size : float, optional\n    Scaling factor for the generator size (default is 0.25).\ngenerator_layer_size : float, optional\n    Scaling factor for the generator layer size (default is 1.0).</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>from_dict(data: Dict[str, Any]) -> 'ModelConfig'\n    Creates an instance of ModelConfig from a dictionary.</p>\n"}, {"fullname": "finlm.config.ModelConfig.__init__", "modulename": "finlm.config", "qualname": "ModelConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">embedding_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">128</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">256</span>,</span><span class=\"param\">\t<span class=\"n\">num_hidden_layers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">12</span>,</span><span class=\"param\">\t<span class=\"n\">num_attention_heads</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">intermediate_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1024</span>,</span><span class=\"param\">\t<span class=\"n\">generator_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.25</span>,</span><span class=\"param\">\t<span class=\"n\">generator_layer_size</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "finlm.config.ModelConfig.embedding_size", "modulename": "finlm.config", "qualname": "ModelConfig.embedding_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "128"}, {"fullname": "finlm.config.ModelConfig.hidden_size", "modulename": "finlm.config", "qualname": "ModelConfig.hidden_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "256"}, {"fullname": "finlm.config.ModelConfig.num_hidden_layers", "modulename": "finlm.config", "qualname": "ModelConfig.num_hidden_layers", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "12"}, {"fullname": "finlm.config.ModelConfig.num_attention_heads", "modulename": "finlm.config", "qualname": "ModelConfig.num_attention_heads", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "4"}, {"fullname": "finlm.config.ModelConfig.intermediate_size", "modulename": "finlm.config", "qualname": "ModelConfig.intermediate_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "1024"}, {"fullname": "finlm.config.ModelConfig.generator_size", "modulename": "finlm.config", "qualname": "ModelConfig.generator_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.25"}, {"fullname": "finlm.config.ModelConfig.generator_layer_size", "modulename": "finlm.config", "qualname": "ModelConfig.generator_layer_size", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "1.0"}, {"fullname": "finlm.config.ModelConfig.from_dict", "modulename": "finlm.config", "qualname": "ModelConfig.from_dict", "kind": "function", "doc": "<p>Creates an instance of ModelConfig from a dictionary.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data : Dict[str, Any]\n    A dictionary containing the configuration parameters.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>ModelConfig\n    An instance of ModelConfig initialized with the provided data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">finlm</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">ModelConfig</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.config.OptimizationConfig", "modulename": "finlm.config", "qualname": "OptimizationConfig", "kind": "class", "doc": "<p>Configuration class for the optimization settings of the FinLM model.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>learning_rate : float, optional\n    Learning rate for the optimizer (default is 0.0001).\nn_epochs : int, optional\n    Number of training epochs (default is 1).\nlr_scheduler_warm_up_steps : int, optional\n    Number of warm-up steps for the learning rate scheduler (default is 1000).\nmlm_probability : float, optional\n    Probability of masking tokens for masked language modeling (default is 0.15).\nuse_gradient_clipping : bool, optional\n    Whether to use gradient clipping (default is True).\ndiscriminator_weight : int, optional\n    Weight for the discriminator loss (default is 50).\ndiscriminator_sampling : str, optional\n    Sampling strategy for the discriminator (default is \"multinomial\").</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>from_dict(data: Dict[str, Any]) -> 'OptimizationConfig'\n    Creates an instance of OptimizationConfig from a dictionary.</p>\n"}, {"fullname": "finlm.config.OptimizationConfig.__init__", "modulename": "finlm.config", "qualname": "OptimizationConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">learning_rate</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0001</span>,</span><span class=\"param\">\t<span class=\"n\">n_epochs</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">lr_scheduler_warm_up_steps</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span>,</span><span class=\"param\">\t<span class=\"n\">mlm_probability</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.15</span>,</span><span class=\"param\">\t<span class=\"n\">use_gradient_clipping</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">discriminator_weight</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">50</span>,</span><span class=\"param\">\t<span class=\"n\">discriminator_sampling</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;multinomial&#39;</span></span>)</span>"}, {"fullname": "finlm.config.OptimizationConfig.learning_rate", "modulename": "finlm.config", "qualname": "OptimizationConfig.learning_rate", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.0001"}, {"fullname": "finlm.config.OptimizationConfig.n_epochs", "modulename": "finlm.config", "qualname": "OptimizationConfig.n_epochs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "1"}, {"fullname": "finlm.config.OptimizationConfig.lr_scheduler_warm_up_steps", "modulename": "finlm.config", "qualname": "OptimizationConfig.lr_scheduler_warm_up_steps", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "1000"}, {"fullname": "finlm.config.OptimizationConfig.mlm_probability", "modulename": "finlm.config", "qualname": "OptimizationConfig.mlm_probability", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float", "default_value": "0.15"}, {"fullname": "finlm.config.OptimizationConfig.use_gradient_clipping", "modulename": "finlm.config", "qualname": "OptimizationConfig.use_gradient_clipping", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": "True"}, {"fullname": "finlm.config.OptimizationConfig.discriminator_weight", "modulename": "finlm.config", "qualname": "OptimizationConfig.discriminator_weight", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": "50"}, {"fullname": "finlm.config.OptimizationConfig.discriminator_sampling", "modulename": "finlm.config", "qualname": "OptimizationConfig.discriminator_sampling", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": "&#x27;multinomial&#x27;"}, {"fullname": "finlm.config.OptimizationConfig.from_dict", "modulename": "finlm.config", "qualname": "OptimizationConfig.from_dict", "kind": "function", "doc": "<p>Creates an instance of OptimizationConfig from a dictionary.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data : Dict[str, Any]\n    A dictionary containing the optimization configuration parameters.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>OptimizationConfig\n    An instance of OptimizationConfig initialized with the provided data.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">finlm</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">OptimizationConfig</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.config.FinLMConfig", "modulename": "finlm.config", "qualname": "FinLMConfig", "kind": "class", "doc": "<p>Configuration class for the FinLM model, combining dataset, model, and optimization configurations.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>dataset_config : DatasetConfig\n    Configuration for the dataset.\nmodel_config : ModelConfig\n    Configuration for the model architecture.\noptimization_config : OptimizationConfig\n    Configuration for the optimization settings.\nsave_models_and_results_to : str\n    Path where models and results should be saved.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>from_yaml(config_file_path: str, save_root_path: str) -> 'FinLMConfig'\n    Loads the configuration from a YAML file.\nto_dict() -> Dict[str, Any]\n    Converts the configuration to a dictionary.\nto_json(file_path: str) -> None\n    Saves the configuration as a JSON file.</p>\n"}, {"fullname": "finlm.config.FinLMConfig.__init__", "modulename": "finlm.config", "qualname": "FinLMConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">dataset_config</span><span class=\"p\">:</span> <span class=\"n\">finlm</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">DatasetConfig</span>,</span><span class=\"param\">\t<span class=\"n\">model_config</span><span class=\"p\">:</span> <span class=\"n\">finlm</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">ModelConfig</span>,</span><span class=\"param\">\t<span class=\"n\">optimization_config</span><span class=\"p\">:</span> <span class=\"n\">finlm</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">OptimizationConfig</span>,</span><span class=\"param\">\t<span class=\"n\">save_models_and_results_to</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "finlm.config.FinLMConfig.dataset_config", "modulename": "finlm.config", "qualname": "FinLMConfig.dataset_config", "kind": "variable", "doc": "<p></p>\n", "annotation": ": finlm.config.DatasetConfig"}, {"fullname": "finlm.config.FinLMConfig.model_config", "modulename": "finlm.config", "qualname": "FinLMConfig.model_config", "kind": "variable", "doc": "<p></p>\n", "annotation": ": finlm.config.ModelConfig"}, {"fullname": "finlm.config.FinLMConfig.optimization_config", "modulename": "finlm.config", "qualname": "FinLMConfig.optimization_config", "kind": "variable", "doc": "<p></p>\n", "annotation": ": finlm.config.OptimizationConfig"}, {"fullname": "finlm.config.FinLMConfig.save_models_and_results_to", "modulename": "finlm.config", "qualname": "FinLMConfig.save_models_and_results_to", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "finlm.config.FinLMConfig.from_yaml", "modulename": "finlm.config", "qualname": "FinLMConfig.from_yaml", "kind": "function", "doc": "<p>Loads the configuration from a YAML file.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>config_file_path : str\n    The path to the YAML file containing the configuration.\nsave_root_path : str\n    The root path where models and results will be saved.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>FinLMConfig\n    An instance of FinLMConfig initialized with the data from the YAML file.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">config_file_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">save_root_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">finlm</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">FinLMConfig</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.config.FinLMConfig.to_dict", "modulename": "finlm.config", "qualname": "FinLMConfig.to_dict", "kind": "function", "doc": "<p>Converts the configuration to a dictionary.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Dict[str, Any]\n    A dictionary representation of the FinLMConfig.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.config.FinLMConfig.to_json", "modulename": "finlm.config", "qualname": "FinLMConfig.to_json", "kind": "function", "doc": "<p>Saves the configuration as a JSON file.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>file_path : str\n    The path where the JSON file will be saved.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">file_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.config.FintuningConfig", "modulename": "finlm.config", "qualname": "FintuningConfig", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "finlm.config.FintuningConfig.__init__", "modulename": "finlm.config", "qualname": "FintuningConfig.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">num_labels</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">tokenizer_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">max_sequence_length</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">text_column</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_columns</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">shuffle_data</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">shuffle_data_random_seed</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">training_data_fraction</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">n_epochs</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">learning_rate</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">classifier_dropout</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">warmup_step_fraction</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">use_gradient_clipping</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "finlm.config.FintuningConfig.model_path", "modulename": "finlm.config", "qualname": "FintuningConfig.model_path", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "finlm.config.FintuningConfig.num_labels", "modulename": "finlm.config", "qualname": "FintuningConfig.num_labels", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "finlm.config.FintuningConfig.tokenizer_path", "modulename": "finlm.config", "qualname": "FintuningConfig.tokenizer_path", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "finlm.config.FintuningConfig.max_sequence_length", "modulename": "finlm.config", "qualname": "FintuningConfig.max_sequence_length", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "finlm.config.FintuningConfig.text_column", "modulename": "finlm.config", "qualname": "FintuningConfig.text_column", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "finlm.config.FintuningConfig.dataset_name", "modulename": "finlm.config", "qualname": "FintuningConfig.dataset_name", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "finlm.config.FintuningConfig.dataset_columns", "modulename": "finlm.config", "qualname": "FintuningConfig.dataset_columns", "kind": "variable", "doc": "<p></p>\n", "annotation": ": list[str]"}, {"fullname": "finlm.config.FintuningConfig.shuffle_data", "modulename": "finlm.config", "qualname": "FintuningConfig.shuffle_data", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool"}, {"fullname": "finlm.config.FintuningConfig.shuffle_data_random_seed", "modulename": "finlm.config", "qualname": "FintuningConfig.shuffle_data_random_seed", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "finlm.config.FintuningConfig.training_data_fraction", "modulename": "finlm.config", "qualname": "FintuningConfig.training_data_fraction", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "finlm.config.FintuningConfig.n_epochs", "modulename": "finlm.config", "qualname": "FintuningConfig.n_epochs", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, Any]"}, {"fullname": "finlm.config.FintuningConfig.learning_rate", "modulename": "finlm.config", "qualname": "FintuningConfig.learning_rate", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, Any]"}, {"fullname": "finlm.config.FintuningConfig.classifier_dropout", "modulename": "finlm.config", "qualname": "FintuningConfig.classifier_dropout", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, Any]"}, {"fullname": "finlm.config.FintuningConfig.warmup_step_fraction", "modulename": "finlm.config", "qualname": "FintuningConfig.warmup_step_fraction", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, Any]"}, {"fullname": "finlm.config.FintuningConfig.use_gradient_clipping", "modulename": "finlm.config", "qualname": "FintuningConfig.use_gradient_clipping", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, Any]"}, {"fullname": "finlm.config.FintuningConfig.save_path", "modulename": "finlm.config", "qualname": "FintuningConfig.save_path", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "finlm.config.FintuningConfig.from_yaml", "modulename": "finlm.config", "qualname": "FintuningConfig.from_yaml", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">config_file_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.config.FintuningConfig.to_dict", "modulename": "finlm.config", "qualname": "FintuningConfig.to_dict", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.config.FintuningConfig.to_json", "modulename": "finlm.config", "qualname": "FintuningConfig.to_json", "kind": "function", "doc": "<p>Saves the configuration as a JSON file.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>file_path : str\n    The path where the JSON file will be saved.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">file_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.dataset", "modulename": "finlm.dataset", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinLMDataset", "modulename": "finlm.dataset", "qualname": "FinLMDataset", "kind": "class", "doc": "<p>A class for defining a dataset which retrieves chunked text sequences, tokenizes them and returns batches of\ninput_ids and attention_mask tensors which are needed for pretraining financial models.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>tokenizer : FinLMTokenizer\n    The tokenizer used to tokenize text sequences.\nmask_token_id : int\n    The ID of the mask token in the tokenizer.\nspecial_token_ids : set\n    A set of IDs corresponding to all special tokens, excluding the mask token.\nmax_sequence_length : int\n    The maximum number of tokens allowed per sequence.\ndb_name : str\n    The path to the SQLite database where text sequences are stored.\ndatabase_retrieval : dict[str, dict[str, int]]\n    A dictionary specifying the retrieval information for each table in the database.\ntable_names : list[str]\n    A list of table names from which sequences are to be retrieved.\nbatch_size : int\n    The number of sequences returned per batch.</p>\n"}, {"fullname": "finlm.dataset.FinLMDataset.__init__", "modulename": "finlm.dataset", "qualname": "FinLMDataset.__init__", "kind": "function", "doc": "<p>Initializes the FinLMDataset with necessary parameters.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>tokenizer_path : str\n    Path to the pretrained tokenizer.\nmax_sequence_length : int\n    The maximum number of tokens allowed per sequence.\ndb_name : str\n    Path to the SQLite database containing chunked text sequences.\ndatabase_retrieval : dict[str, dict[str, int]]\n    A dictionary specifying retrieval instructions for each table in the database.\nbatch_size : int\n    The number of sequences to return in each batch.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tokenizer_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">max_sequence_length</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">db_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">database_retrieval</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span>)</span>"}, {"fullname": "finlm.dataset.FinLMDataset.logger", "modulename": "finlm.dataset", "qualname": "FinLMDataset.logger", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinLMDataset.tokenizer", "modulename": "finlm.dataset", "qualname": "FinLMDataset.tokenizer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinLMDataset.mask_token_id", "modulename": "finlm.dataset", "qualname": "FinLMDataset.mask_token_id", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinLMDataset.special_token_ids", "modulename": "finlm.dataset", "qualname": "FinLMDataset.special_token_ids", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinLMDataset.max_sequence_length", "modulename": "finlm.dataset", "qualname": "FinLMDataset.max_sequence_length", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinLMDataset.db_name", "modulename": "finlm.dataset", "qualname": "FinLMDataset.db_name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinLMDataset.database_retrieval", "modulename": "finlm.dataset", "qualname": "FinLMDataset.database_retrieval", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinLMDataset.table_names", "modulename": "finlm.dataset", "qualname": "FinLMDataset.table_names", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinLMDataset.batch_size", "modulename": "finlm.dataset", "qualname": "FinLMDataset.batch_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinLMDataset.from_dict", "modulename": "finlm.dataset", "qualname": "FinLMDataset.from_dict", "kind": "function", "doc": "<p>Instantiates the <code>FinLMDataset</code> class from a configuration dictionary.</p>\n\n<p>This class method allows the creation of a <code>FinLMDataset</code> object using a dictionary that contains\nall the necessary parameters for initialization.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data_config : Dict[str, Any]\n    A dictionary where keys correspond to the parameters required by the <code>__init__</code> method.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>FinLMDataset\n    An instance of the <code>FinLMDataset</code> class.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">data_config</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">finlm</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">FinLMDataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.dataset.FinLMDataset.prepare_data_loader", "modulename": "finlm.dataset", "qualname": "FinLMDataset.prepare_data_loader", "kind": "function", "doc": "<p>Prepares the data loader by retrieving sequences and creating a Hugging Face dataset.</p>\n\n<p>This method first retrieves sequences from the database using <code>_retrieve_sequences_from_database</code> \nand then tokenizes them by calling <code>_create_hf_dataset</code>. The result is a PyTorch DataLoader \nready for iteration during training.</p>\n\n<h2 id=\"attributes-updated\">Attributes Updated</h2>\n\n<p>data_loader : torch.utils.data.DataLoader\n    A PyTorch DataLoader created from the tokenized dataset.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.dataset.FinLMDataset.set_dataset_offsets", "modulename": "finlm.dataset", "qualname": "FinLMDataset.set_dataset_offsets", "kind": "function", "doc": "<p>Updates the offsets in the <code>database_retrieval</code> dictionary based on the current epoch.</p>\n\n<p>This method adjusts the <code>offset</code> in the <code>database_retrieval</code> dictionary for each table according \nto the current epoch. If the offset exceeds the total number of sequences available, it resets \nthe offset to zero and logs a message.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>epoch : int\n    The current epoch number, used to calculate the new offset for sequence retrieval.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">epoch</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.dataset.FinetuningDataset", "modulename": "finlm.dataset", "qualname": "FinetuningDataset", "kind": "class", "doc": "<p>A dataset class designed for fine-tuning language models.</p>\n\n<p>This class handles the preparation of datasets for fine-tuning, including tokenization,\nshuffling, and splitting into training and test datasets.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>tokenizer_path : str\n    Path to the pretrained tokenizer to be used for tokenization.\ntokenizer : FinLMTokenizer\n    The tokenizer instance used for tokenizing the text sequences.\nmax_sequence_length : int\n    The maximum number of tokens per sequence.\ndataset : Dataset\n    The Hugging Face <code>Dataset</code> object containing the raw data to be processed.\ntext_column : str\n    The name of the column in the dataset containing the text sequences to be tokenized.\ndataset_columns : list[str]\n    A list of column names to include in the final tokenized dataset.\nshuffle_data : bool\n    A flag indicating whether the dataset should be shuffled.\nshuffle_data_random_seed : Optional[int]\n    The random seed to use for shuffling, if applicable.\ntraining_data_fraction : float\n    The fraction of the dataset to use for training; the rest is used for testing.\ntrain_size : int\n    The number of samples in the training dataset.\ntraining_data : Dataset\n    The tokenized and processed training dataset.\ntest_data : Dataset\n    The tokenized and processed test dataset.\nlogger : logging.Logger\n    Logger instance for logging messages.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>_tokenization(text_sequence: list[str]) -> dict\n    Tokenizes a list of text sequences using the pretrained tokenizer.\n_map_dataset() -> None\n    Tokenizes the entire dataset and sets the format to be compatible with PyTorch.\n_prepare_training_and_test_data() -> None\n    Tokenizes, shuffles, and splits the dataset into training and test datasets.</p>\n"}, {"fullname": "finlm.dataset.FinetuningDataset.__init__", "modulename": "finlm.dataset", "qualname": "FinetuningDataset.__init__", "kind": "function", "doc": "<p>Initializes the <code>FinetuningDataset</code> with the necessary parameters.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>tokenizer_path : str\n    Path to the pretrained tokenizer to be used for tokenization.\nmax_sequence_length : int\n    The maximum number of tokens per sequence.\ndataset : Dataset\n    The Hugging Face <code>Dataset</code> object containing the raw data to be processed.\ntext_column : str\n    The name of the column in the dataset containing the text sequences to be tokenized.\ndataset_columns : list[str]\n    A list of column names to include in the final tokenized dataset.\nshuffle_data : bool, optional\n    Whether to shuffle the dataset before splitting (default is True).\nshuffle_data_random_seed : Optional[int], optional\n    Random seed for shuffling the dataset, if applicable (default is None).\ntraining_data_fraction : float, optional\n    Fraction of the dataset to use for training (default is 0.80).</p>\n\n<h2 id=\"attributes-initialized\">Attributes Initialized</h2>\n\n<p>tokenizer : FinLMTokenizer\n    The tokenizer instance used for tokenizing the text sequences.\ntrain_size : int\n    The number of samples in the training dataset.\ntraining_data : Dataset\n    The tokenized and processed training dataset.\ntest_data : Dataset\n    The tokenized and processed test dataset.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tokenizer_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">max_sequence_length</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">arrow_dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>,</span><span class=\"param\">\t<span class=\"n\">text_column</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_columns</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">shuffle_data</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">shuffle_data_random_seed</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">training_data_fraction</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.8</span></span>)</span>"}, {"fullname": "finlm.dataset.FinetuningDataset.tokenizer_path", "modulename": "finlm.dataset", "qualname": "FinetuningDataset.tokenizer_path", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinetuningDataset.tokenizer", "modulename": "finlm.dataset", "qualname": "FinetuningDataset.tokenizer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinetuningDataset.max_sequence_length", "modulename": "finlm.dataset", "qualname": "FinetuningDataset.max_sequence_length", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinetuningDataset.dataset", "modulename": "finlm.dataset", "qualname": "FinetuningDataset.dataset", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinetuningDataset.text_column", "modulename": "finlm.dataset", "qualname": "FinetuningDataset.text_column", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinetuningDataset.dataset_columns", "modulename": "finlm.dataset", "qualname": "FinetuningDataset.dataset_columns", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinetuningDataset.shuffle_data", "modulename": "finlm.dataset", "qualname": "FinetuningDataset.shuffle_data", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinetuningDataset.shuffle_data_random_seed", "modulename": "finlm.dataset", "qualname": "FinetuningDataset.shuffle_data_random_seed", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinetuningDataset.training_data_fraction", "modulename": "finlm.dataset", "qualname": "FinetuningDataset.training_data_fraction", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.FinetuningDataset.logger", "modulename": "finlm.dataset", "qualname": "FinetuningDataset.logger", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.AggregatedDocumentDataset", "modulename": "finlm.dataset", "qualname": "AggregatedDocumentDataset", "kind": "class", "doc": "<p>A custom PyTorch dataset class for handling documents composed of multiple sequences, where each document has an associated label.</p>\n\n<p>This dataset supports tokenization of sequences within each document, and can optionally be used with a DataLoader for batching and shuffling.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>documents : list[list[str]]\n    A list where each element is a document, and each document is a list of sequences (strings).\nlabels : list\n    A list of labels, one for each document.\ntokenizer_path : str\n    Path to the pretrained tokenizer to be used for tokenization.\ntokenizer : FinLMTokenizer\n    The tokenizer instance used for tokenizing the text sequences.\nsequence_length : int\n    The length to which each sequence should be padded or truncated.\nuse_dataloader : bool\n    Whether to use a PyTorch DataLoader for batching and shuffling.\ndataloader : DataLoader or None\n    A PyTorch DataLoader instance if <code>use_dataloader</code> is True; otherwise, None.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>__len__()\n    Returns the number of documents in the dataset.\n__getitem__(idx)\n    Retrieves and tokenizes a document and its label based on the provided index.\n__iter__()\n    Returns an iterator over the dataset.\n__next__()\n    Retrieves the next item in the dataset when not using a DataLoader.\n_collate_fn(batch)\n    A static method that collates a batch of documents into a format suitable for model input.</p>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "finlm.dataset.AggregatedDocumentDataset.__init__", "modulename": "finlm.dataset", "qualname": "AggregatedDocumentDataset.__init__", "kind": "function", "doc": "<p>Initializes the <code>AggregatedDocumentDataset</code> with documents, labels, and tokenizer settings.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>documents : list[list[str]]\n    A list where each element is a document, and each document is a list of sequences (strings).\nlabels : list\n    A list of labels, one for each document.\ntokenizer_path : str\n    Path to the pretrained tokenizer to be used for tokenization.\nsequence_length : int\n    The length to which each sequence should be padded or truncated.\nuse_dataloader : bool, optional\n    Whether to use a PyTorch DataLoader for batching and shuffling (default is False).\nshuffle : bool, optional\n    Whether to shuffle the data if using a DataLoader (default is False).\nbatch_size : int, optional\n    The batch size to use if using a DataLoader (default is 1).</p>\n\n<h2 id=\"attributes-initialized\">Attributes Initialized</h2>\n\n<p>dataloader : DataLoader or None\n    A PyTorch DataLoader instance if <code>use_dataloader</code> is True; otherwise, None.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">documents</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span>,</span><span class=\"param\">\t<span class=\"n\">tokenizer_path</span>,</span><span class=\"param\">\t<span class=\"n\">sequence_length</span>,</span><span class=\"param\">\t<span class=\"n\">use_dataloader</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">1</span></span>)</span>"}, {"fullname": "finlm.dataset.AggregatedDocumentDataset.documents", "modulename": "finlm.dataset", "qualname": "AggregatedDocumentDataset.documents", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.AggregatedDocumentDataset.labels", "modulename": "finlm.dataset", "qualname": "AggregatedDocumentDataset.labels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.AggregatedDocumentDataset.tokenizer_path", "modulename": "finlm.dataset", "qualname": "AggregatedDocumentDataset.tokenizer_path", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.AggregatedDocumentDataset.tokenizer", "modulename": "finlm.dataset", "qualname": "AggregatedDocumentDataset.tokenizer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.AggregatedDocumentDataset.sequence_length", "modulename": "finlm.dataset", "qualname": "AggregatedDocumentDataset.sequence_length", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.dataset.AggregatedDocumentDataset.use_dataloader", "modulename": "finlm.dataset", "qualname": "AggregatedDocumentDataset.use_dataloader", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming", "modulename": "finlm.downstreaming", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.Hyperparameter", "modulename": "finlm.downstreaming", "qualname": "Hyperparameter", "kind": "class", "doc": "<p>A class representing a hyperparameter with a specified type and optional range.</p>\n\n<p>This class is used to define hyperparameters that can be used in various machine \nlearning or deep learning models. Each hyperparameter has a name, data type, and \noptional range (low, high) along with a default value.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>name : str\n    The name of the hyperparameter.\ndtype : type\n    The data type of the hyperparameter (e.g., int, float, bool).\nlow : Optional[Union[int, float]], optional\n    The lower bound for the hyperparameter (default is None).\nhigh : Optional[Union[int, float]], optional\n    The upper bound for the hyperparameter (default is None).\ndefault : Optional[Union[int, float, bool]], optional\n    The default value of the hyperparameter (default is None).</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>from_dict(data: Dict[str, Any]) -> 'Hyperparameter'\n    Creates an instance of the Hyperparameter class from a dictionary.</p>\n"}, {"fullname": "finlm.downstreaming.Hyperparameter.__init__", "modulename": "finlm.downstreaming", "qualname": "Hyperparameter.__init__", "kind": "function", "doc": "<p>Initializes a Hyperparameter instance with the specified attributes.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>name : str\n    The name of the hyperparameter.\ndtype : type\n    The data type of the hyperparameter (e.g., int, float, bool).\nlow : Optional[Union[int, float]], optional\n    The lower bound for the hyperparameter (default is None).\nhigh : Optional[Union[int, float]], optional\n    The upper bound for the hyperparameter (default is None).\ndefault : Optional[Union[int, float, bool]], optional\n    The default value of the hyperparameter (default is None).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">name</span>, </span><span class=\"param\"><span class=\"n\">dtype</span>, </span><span class=\"param\"><span class=\"n\">low</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">high</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">default</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "finlm.downstreaming.Hyperparameter.name", "modulename": "finlm.downstreaming", "qualname": "Hyperparameter.name", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.Hyperparameter.dtype", "modulename": "finlm.downstreaming", "qualname": "Hyperparameter.dtype", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.Hyperparameter.low", "modulename": "finlm.downstreaming", "qualname": "Hyperparameter.low", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.Hyperparameter.high", "modulename": "finlm.downstreaming", "qualname": "Hyperparameter.high", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.Hyperparameter.default", "modulename": "finlm.downstreaming", "qualname": "Hyperparameter.default", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.Hyperparameter.from_dict", "modulename": "finlm.downstreaming", "qualname": "Hyperparameter.from_dict", "kind": "function", "doc": "<p>Creates an instance of the Hyperparameter class from a dictionary.</p>\n\n<p>This method converts the data type string to the corresponding Python type \n(e.g., \"int\" to int) and initializes the Hyperparameter instance with the \nprovided values.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>data : Dict[str, Any]\n    A dictionary containing the configuration parameters.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Hyperparameter\n    An instance of Hyperparameter initialized with the provided data.</p>\n\n<h2 id=\"raises\">Raises</h2>\n\n<p>ValueError\n    If the dtype in the dictionary is not supported.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">finlm</span><span class=\"o\">.</span><span class=\"n\">downstreaming</span><span class=\"o\">.</span><span class=\"n\">Hyperparameter</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier", "kind": "class", "doc": "<p>A class for fine-tuning an encoder-based classifier model, specifically for sequence classification tasks using the ELECTRA architecture.</p>\n\n<p>This class supports tasks such as regression, binary classification, and multi-class classification.\nIt provides functionality for training the model with cross-validation, hyperparameter optimization using Optuna, \nand final evaluation of the trained model.</p>\n\n<p>The methods are written bottom to top which means the first method here, is the one which uses the other methods below to find the \nbest set of hyperparameters for a cross validated training data set. Once the optimization is finished the best model and its \nperformance metrics as well as the configuation of the best model search are saved.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>config : FintuningConfig\n    The configuration object containing hyperparameters and paths required for fine-tuning.\ndevice : torch.device\n    The device (CPU or GPU) on which the model will be trained and evaluated.\ndataset : FinetuningDataset\n    The dataset object used for fine-tuning, created from the input dataset and config.\nmodel_path : str\n    Path to the pre-trained model that will be fine-tuned.\nnum_labels : int\n    Number of labels for the classification task.\ntask : str\n    The type of task being performed: \"regression\", \"binary_classification\", or \"multi_classification\".\nn_epochs : Hyperparameter\n    The number of training epochs, represented as a Hyperparameter object.\nlearning_rate : Hyperparameter\n    The learning rate for the optimizer, represented as a Hyperparameter object.\nclassifier_dropout : Hyperparameter\n    The dropout rate used in the classifier layer, represented as a Hyperparameter object.\nwarmup_step_fraction : Hyperparameter\n    The fraction of warmup steps during training, represented as a Hyperparameter object.\nuse_gradient_clipping : Hyperparameter\n    A boolean indicating whether to use gradient clipping, represented as a Hyperparameter object.\nsave_path : str\n    The directory where the trained model and other outputs will be saved.\nlogger : logging.Logger\n    Logger for recording information during training and evaluation.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>train_optuna_optimized_cv_model(n_trials: int)\n    Trains the model using cross-validation with hyperparameters optimized by Optuna.\noptuna_optimize(n_trials: int = 10) -> Tuple[Dict[str, Any], float]\n    Optimizes hyperparameters using Optuna and returns the best parameters and score.\noptuna_objective(trial) -> float\n    Defines the objective function for Optuna hyperparameter optimization.\ncross_validate(\n    n_folds: int, \n    training_data, \n    training_batch_size: int, \n    validation_batch_size: int, \n    n_epochs: int, \n    learning_rate: float, \n    classifier_dropout: float, \n    warmup_step_fraction: float, \n    use_gradient_clipping: bool\n) -> float\n    Performs cross-validation on the training data and returns the average score across folds.\ntrain(\n    training_data, \n    validation_data, \n    n_epochs: int, \n    learning_rate: float, \n    classifier_dropout: float, \n    warmup_step_fraction: float, \n    use_gradient_clipping: bool, \n    save_best_model_path: str\n) -> float\n    Trains the model on the training data and validates it on the validation data, with early stopping and checkpointing.\nfinal_evaluation(finetuned_model_path: str) -> Tuple[Dict[str, Any], Dict[str, Any]]\n    Evaluates the final trained model on both the training and test datasets, returning performance metrics.\n_determine_scores(true_labels: np.ndarray, predicted_labels: np.ndarray) -> Dict[str, Any]\n    Computes and logs performance metrics based on the true and predicted labels.\n_load_electra_model(classifier_dropout: float, save_path: str = None) -> ElectraForSequenceClassification\n    Loads the ELECTRA model with the specified dropout rate, optionally from a saved checkpoint.</p>\n"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.__init__", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.__init__", "kind": "function", "doc": "<p>Initializes the FinetuningEncoderClassifier with the provided configuration, device, and dataset.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>config : FintuningConfig\n    The configuration object containing hyperparameters and paths required for fine-tuning.\ndevice : torch.device\n    The device (CPU or GPU) on which the model will be trained and evaluated.\ndataset : Dataset\n    The dataset to be used for fine-tuning.</p>\n\n<h2 id=\"raises\">Raises</h2>\n\n<p>ValueError\n    If the save path already exists, indicating the model has already been trained.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">finlm</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">FintuningConfig</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">dataset</span><span class=\"p\">:</span> <span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">arrow_dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span></span>)</span>"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.config", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.model_path", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.model_path", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.num_labels", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.num_labels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.device", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.device", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.dataset", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.dataset", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.n_epochs", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.n_epochs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.learning_rate", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.learning_rate", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.classifier_dropout", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.classifier_dropout", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.warmup_step_fraction", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.warmup_step_fraction", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.use_gradient_clipping", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.use_gradient_clipping", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.save_path", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.save_path", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.logger", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.logger", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.train_optuna_optimized_cv_model", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.train_optuna_optimized_cv_model", "kind": "function", "doc": "<p>Trains the model using cross-validation with hyperparameters optimized by Optuna.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>n_trials : int\n    The number of trials for Optuna optimization.</p>\n\n<p>This method:</p>\n\n<ul>\n<li>Optimizes hyperparameters using Optuna.</li>\n<li>Trains the model on the full training dataset.</li>\n<li>Saves the final trained model and evaluation metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">n_trials</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.optuna_optimize", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.optuna_optimize", "kind": "function", "doc": "<p>Optimizes hyperparameters using Optuna and returns the best parameters and score.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>n_trials : int, optional\n    The number of trials for Optuna optimization (default is 10).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Tuple[Dict[str, Any], float]\n    The best hyperparameters and the corresponding score.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">n_trials</span><span class=\"o\">=</span><span class=\"mi\">10</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.optuna_objective", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.optuna_objective", "kind": "function", "doc": "<p>Defines the objective function for Optuna hyperparameter optimization.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>trial : optuna.Trial\n    A single trial object for Optuna optimization.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>float\n    The cross-validation score for the current trial's hyperparameters.</p>\n\n<h2 id=\"raises\">Raises</h2>\n\n<p>ValueError\n    If the data type of a hyperparameter is not one of float, int, or bool.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">trial</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.cross_validate", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.cross_validate", "kind": "function", "doc": "<p>Performs cross-validation on the training data and returns the average score across folds.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>n_folds : int\n    The number of cross-validation folds.\ntraining_data : Dataset\n    The training dataset to be used.\ntraining_batch_size : int\n    The batch size for training.\nvalidation_batch_size : int\n    The batch size for validation.\nn_epochs : int\n    The number of epochs to train for.\nlearning_rate : float\n    The learning rate for the optimizer.\nclassifier_dropout : float\n    The dropout rate for the classifier layer.\nwarmup_step_fraction : float\n    The fraction of steps for learning rate warm-up.\nuse_gradient_clipping : bool\n    Whether to apply gradient clipping.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>float\n    The average cross-validation score across all folds.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">n_folds</span>,</span><span class=\"param\">\t<span class=\"n\">training_data</span>,</span><span class=\"param\">\t<span class=\"n\">training_batch_size</span>,</span><span class=\"param\">\t<span class=\"n\">validation_batch_size</span>,</span><span class=\"param\">\t<span class=\"n\">n_epochs</span>,</span><span class=\"param\">\t<span class=\"n\">learning_rate</span>,</span><span class=\"param\">\t<span class=\"n\">classifier_dropout</span>,</span><span class=\"param\">\t<span class=\"n\">warmup_step_fraction</span>,</span><span class=\"param\">\t<span class=\"n\">use_gradient_clipping</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.train", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.train", "kind": "function", "doc": "<p>Trains the model on the training data and validates it on the validation data, with early stopping and checkpointing.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>training_data : DataLoader\n    The training data loader.\nvalidation_data : DataLoader\n    The validation data loader.\nn_epochs : int\n    The number of epochs to train for.\nlearning_rate : float\n    The learning rate for the optimizer.\nclassifier_dropout : float\n    The dropout rate for the classifier layer.\nwarmup_step_fraction : float\n    The fraction of steps for learning rate warm-up.\nuse_gradient_clipping : bool\n    Whether to apply gradient clipping.\nsave_best_model_path : str\n    The path to save the best model during training.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>float\n    The best validation score achieved during training.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">training_data</span>,</span><span class=\"param\">\t<span class=\"n\">validation_data</span>,</span><span class=\"param\">\t<span class=\"n\">n_epochs</span>,</span><span class=\"param\">\t<span class=\"n\">learning_rate</span>,</span><span class=\"param\">\t<span class=\"n\">classifier_dropout</span>,</span><span class=\"param\">\t<span class=\"n\">warmup_step_fraction</span>,</span><span class=\"param\">\t<span class=\"n\">use_gradient_clipping</span>,</span><span class=\"param\">\t<span class=\"n\">save_best_model_path</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.downstreaming.FinetuningEncoderClassifier.final_evaluation", "modulename": "finlm.downstreaming", "qualname": "FinetuningEncoderClassifier.final_evaluation", "kind": "function", "doc": "<p>Evaluates the final trained model on both the training and test datasets, returning performance metrics.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>finetuned_model_path : str\n    The path to the fine-tuned model for evaluation.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Tuple[Dict[str, Any], Dict[str, Any]]\n    The training and test performance metrics.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">fintuned_model_path</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models", "modulename": "finlm.models", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "finlm.models.PretrainLM", "modulename": "finlm.models", "qualname": "PretrainLM", "kind": "class", "doc": "<p>A class for pretraining a language model using the configurations provided in FinLMConfig.</p>\n\n<p>This class handles the setup of the dataset, model configurations, and optimization settings \nfor pretraining a language model. It also includes utility methods for token masking and \ndirectory management.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>config : FinLMConfig\n    Configuration object containing dataset, model, and optimization configurations.\ndataset_config : DatasetConfig\n    Configuration for the dataset.\nmodel_config : ModelConfig\n    Configuration for the model architecture.\noptimization_config : OptimizationConfig\n    Configuration for the optimization settings.\nsave_root_path : str\n    Path where models and results will be saved.\nlogger : logging.Logger\n    Logger instance for logging messages related to the pretraining process.\ndevice : torch.device\n    Device on which computations will be performed (CPU or CUDA).</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>load_dataset() -> None\n    Loads the dataset based on the configuration.\nmask_tokens(inputs, mlm_probability, mask_token_id, special_token_ids, n_tokens, ignore_index=-100, hard_masking=False) -> Tuple[torch.Tensor, torch.Tensor]\n    Applies masked language modeling (MLM) to the input tokens.\n_create_directory_and_return_save_path(model_type: str) -> str\n    Creates a directory for saving the model and returns the path.\n_set_device() -> None\n    Sets the device to CUDA if available; otherwise, defaults to CPU.</p>\n"}, {"fullname": "finlm.models.PretrainLM.__init__", "modulename": "finlm.models", "qualname": "PretrainLM.__init__", "kind": "function", "doc": "<p>Initializes the PretrainLM class with the given configuration.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>config : FinLMConfig\n    Configuration object containing dataset, model, and optimization configurations.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"n\">finlm</span><span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">.</span><span class=\"n\">FinLMConfig</span></span>)</span>"}, {"fullname": "finlm.models.PretrainLM.config", "modulename": "finlm.models", "qualname": "PretrainLM.config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.PretrainLM.dataset_config", "modulename": "finlm.models", "qualname": "PretrainLM.dataset_config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.PretrainLM.model_config", "modulename": "finlm.models", "qualname": "PretrainLM.model_config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.PretrainLM.optimization_config", "modulename": "finlm.models", "qualname": "PretrainLM.optimization_config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.PretrainLM.save_root_path", "modulename": "finlm.models", "qualname": "PretrainLM.save_root_path", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.PretrainLM.logger", "modulename": "finlm.models", "qualname": "PretrainLM.logger", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.PretrainLM.load_dataset", "modulename": "finlm.models", "qualname": "PretrainLM.load_dataset", "kind": "function", "doc": "<p>Loads the dataset based on the dataset configuration.</p>\n\n<p>This method initializes the FinLMDataset using the dataset configuration provided in \nthe FinLMConfig object.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainLM.mask_tokens", "modulename": "finlm.models", "qualname": "PretrainLM.mask_tokens", "kind": "function", "doc": "<p>Applies masked language modeling (MLM) to the input tokens.</p>\n\n<p>This method randomly masks a portion of the input tokens according to the specified \nprobability, and optionally replaces some tokens with random words or keeps them unchanged.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>inputs : torch.Tensor\n    Tensor containing the input token IDs.\nmlm_probability : float\n    Probability of masking a token for MLM.\nmask_token_id : int\n    The token ID to use for masking (typically the ID for the [MASK] token).\nspecial_token_ids : list[int]\n    List of token IDs that should not be masked (e.g., special tokens like [CLS], [SEP]).\nn_tokens : int\n    The total number of tokens in the vocabulary (used for selecting random tokens).\nignore_index : int, optional\n    The index to ignore in the loss calculation (default is -100).\nhard_masking : bool, optional\n    If True, all masked tokens are replaced by the mask token; otherwise, some tokens may be \n    replaced by random tokens or left unchanged (default is False).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Tuple[torch.Tensor, torch.Tensor]\n    A tuple containing the masked input tensor and the corresponding labels tensor.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">inputs</span>,</span><span class=\"param\">\t<span class=\"n\">mlm_probability</span>,</span><span class=\"param\">\t<span class=\"n\">mask_token_id</span>,</span><span class=\"param\">\t<span class=\"n\">special_token_ids</span>,</span><span class=\"param\">\t<span class=\"n\">n_tokens</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_index</span><span class=\"o\">=-</span><span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">hard_masking</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainMLM", "modulename": "finlm.models", "qualname": "PretrainMLM", "kind": "class", "doc": "<p>A class for pretraining a Masked Language Model (MLM) using the FinLM framework.</p>\n\n<p>This class inherits from <code>PretrainLM</code> and provides specific implementations for \npreparing data, loading the model, and training the Masked Language Model (MLM). </p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>config : FinLMConfig\n    Configuration object containing dataset, model, and optimization configurations.\ndataset : FinLMDataset\n    The dataset prepared for MLM training.\nmodel : ElectraForMaskedLM\n    The Electra model configured for masked language modeling.\noptimizer : torch.optim.Optimizer\n    The optimizer used for training.\nscheduler : torch.optim.lr_scheduler.LambdaLR\n    The learning rate scheduler used during training.\niteration_steps_per_epoch : int\n    Number of iteration steps per training epoch.\nlogger : logging.Logger\n    Logger instance for logging messages related to training.\ndevice : torch.device\n    Device on which computations will be performed (CPU or CUDA).</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>load_model() -> None\n    Loads and configures the Electra model for masked language modeling.\nload_optimization() -> None\n    Sets up the optimizer and learning rate scheduler based on the optimization configuration.\nprepare_data_model_optimizer() -> None\n    Prepares the dataset, model, and optimizer for training.\ntrain() -> None\n    Trains the masked language model and saves the results and model.</p>\n", "bases": "PretrainLM"}, {"fullname": "finlm.models.PretrainMLM.__init__", "modulename": "finlm.models", "qualname": "PretrainMLM.__init__", "kind": "function", "doc": "<p>Initializes the PretrainMLM class with the given configuration.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>config : FinLMConfig\n    Configuration object containing dataset, model, and optimization configurations.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">config</span></span>)</span>"}, {"fullname": "finlm.models.PretrainMLM.load_model", "modulename": "finlm.models", "qualname": "PretrainMLM.load_model", "kind": "function", "doc": "<p>Loads and configures the Electra model for masked language modeling.</p>\n\n<p>This method initializes the Electra model using the configuration settings, \nincluding vocabulary size, embedding size, hidden size, and other model parameters. \nThe model is then moved to the appropriate device (CPU or GPU).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainMLM.load_optimization", "modulename": "finlm.models", "qualname": "PretrainMLM.load_optimization", "kind": "function", "doc": "<p>Sets up the optimizer and learning rate scheduler based on the optimization configuration.</p>\n\n<p>This method calculates the total number of training steps, initializes the AdamW optimizer, \nand configures a linear learning rate scheduler with warm-up steps.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainMLM.prepare_data_model_optimizer", "modulename": "finlm.models", "qualname": "PretrainMLM.prepare_data_model_optimizer", "kind": "function", "doc": "<p>Prepares the dataset, model, and optimizer for training.</p>\n\n<p>This method calls the appropriate methods to load the dataset, load the model, \nand set up the optimizer and learning rate scheduler.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainMLM.train", "modulename": "finlm.models", "qualname": "PretrainMLM.train", "kind": "function", "doc": "<p>Trains the masked language model and saves the results and model.</p>\n\n<p>This method handles the training loop, including masking input tokens, calculating \nthe MLM loss, updating model parameters, and logging training metrics. After training \nis complete, it saves the model, training metrics, and plots of the loss and accuracy.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainDiscriminator", "modulename": "finlm.models", "qualname": "PretrainDiscriminator", "kind": "class", "doc": "<p>A class for pretraining a discriminator model in the Electra framework using the FinLM setup.</p>\n\n<p>This class inherits from <code>PretrainLM</code> and provides specific implementations for \npreparing data, loading the discriminator model, and training the model.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>config : FinLMConfig\n    Configuration object containing dataset, model, and optimization configurations.\ndataset : FinLMDataset\n    The dataset prepared for discriminator training.\nmodel : ElectraForPreTraining\n    The Electra model configured for discriminator pretraining.\noptimizer : torch.optim.Optimizer\n    The optimizer used for training.\nscheduler : torch.optim.lr_scheduler.LambdaLR\n    The learning rate scheduler used during training.\niteration_steps_per_epoch : int\n    Number of iteration steps per training epoch.\nlogger : logging.Logger\n    Logger instance for logging messages related to training.\ndevice : torch.device\n    Device on which computations will be performed (CPU or CUDA).</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>load_model() -> None\n    Loads and configures the Electra discriminator model.\nload_optimization() -> None\n    Sets up the optimizer and learning rate scheduler based on the optimization configuration.\nprepare_data_model_optimizer() -> None\n    Prepares the dataset, model, and optimizer for training.\nreplace_masked_tokens_randomly(inputs: torch.Tensor, mlm_probability: float, mask_token_id: int, special_token_ids: list[int], n_tokens: int, hard_masking: bool = True) -> Tuple[torch.Tensor, torch.Tensor]\n    Replaces masked tokens with random tokens and generates labels for discriminator training.\ntrain() -> None\n    Trains the discriminator model and saves the results and model.</p>\n", "bases": "PretrainLM"}, {"fullname": "finlm.models.PretrainDiscriminator.__init__", "modulename": "finlm.models", "qualname": "PretrainDiscriminator.__init__", "kind": "function", "doc": "<p>Initializes the PretrainDiscriminator class with the given configuration.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>config : FinLMConfig\n    Configuration object containing dataset, model, and optimization configurations.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">config</span></span>)</span>"}, {"fullname": "finlm.models.PretrainDiscriminator.load_model", "modulename": "finlm.models", "qualname": "PretrainDiscriminator.load_model", "kind": "function", "doc": "<p>Loads and configures the Electra discriminator model.</p>\n\n<p>This method initializes the Electra model for discriminator pretraining using the configuration settings, \nincluding vocabulary size, embedding size, hidden size, and other model parameters. The model is then moved \nto the appropriate device (CPU or GPU).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainDiscriminator.load_optimization", "modulename": "finlm.models", "qualname": "PretrainDiscriminator.load_optimization", "kind": "function", "doc": "<p>Sets up the optimizer and learning rate scheduler based on the optimization configuration.</p>\n\n<p>This method calculates the total number of training steps, initializes the AdamW optimizer, \nand configures a linear learning rate scheduler with warm-up steps.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainDiscriminator.prepare_data_model_optimizer", "modulename": "finlm.models", "qualname": "PretrainDiscriminator.prepare_data_model_optimizer", "kind": "function", "doc": "<p>Prepares the dataset, model, and optimizer for training.</p>\n\n<p>This method calls the appropriate methods to load the dataset, load the model, \nand set up the optimizer and learning rate scheduler.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainDiscriminator.replace_masked_tokens_randomly", "modulename": "finlm.models", "qualname": "PretrainDiscriminator.replace_masked_tokens_randomly", "kind": "function", "doc": "<p>Replaces masked tokens with random tokens and generates labels for discriminator training.</p>\n\n<p>This method first applies masked language modeling (MLM) to the input tokens. It then replaces \nthe masked tokens with random tokens and generates labels indicating whether a token has been \nreplaced (1) or not (0).</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>inputs : torch.Tensor\n    Tensor containing the input token IDs.\nmlm_probability : float\n    Probability of masking a token for MLM.\nmask_token_id : int\n    The token ID to use for masking (typically the ID for the [MASK] token).\nspecial_token_ids : list[int]\n    List of token IDs that should not be masked (e.g., special tokens like [CLS], [SEP]).\nn_tokens : int\n    The total number of tokens in the vocabulary (used for selecting random tokens).\nhard_masking : bool, optional\n    If True, all masked tokens are replaced by the mask token; otherwise, some tokens may be \n    replaced by random tokens or left unchanged (default is True).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Tuple[torch.Tensor, torch.Tensor]\n    A tuple containing the corrupted input tensor and the corresponding labels tensor.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">inputs</span>,</span><span class=\"param\">\t<span class=\"n\">mlm_probability</span>,</span><span class=\"param\">\t<span class=\"n\">mask_token_id</span>,</span><span class=\"param\">\t<span class=\"n\">special_token_ids</span>,</span><span class=\"param\">\t<span class=\"n\">n_tokens</span>,</span><span class=\"param\">\t<span class=\"n\">hard_masking</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainDiscriminator.train", "modulename": "finlm.models", "qualname": "PretrainDiscriminator.train", "kind": "function", "doc": "<p>Trains the discriminator model and saves the results and model.</p>\n\n<p>This method handles the training loop, including replacing masked tokens, calculating \nthe discriminator loss, updating model parameters, and logging training metrics. After \ntraining is complete, it saves the model, training metrics, and plots of the loss, accuracy, \nprecision, and recall.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainElectra", "modulename": "finlm.models", "qualname": "PretrainElectra", "kind": "class", "doc": "<p>A class for pretraining the Electra model using the FinLM setup.</p>\n\n<p>This class inherits from <code>PretrainLM</code> and provides specific implementations for \npreparing data, loading both the generator and discriminator models, and training \nthe Electra model, which includes both components.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>config : FinLMConfig\n    Configuration object containing dataset, model, and optimization configurations.\ndataset : FinLMDataset\n    The dataset prepared for Electra model training.\ngenerator : ElectraForMaskedLM\n    The generator model in the Electra framework configured for masked language modeling.\ndiscriminator : ElectraForPreTraining\n    The discriminator model in the Electra framework configured for identifying replaced tokens.\noptimizer : torch.optim.Optimizer\n    The optimizer used for training.\nscheduler : torch.optim.lr_scheduler.LambdaLR\n    The learning rate scheduler used during training.\niteration_steps_per_epoch : int\n    Number of iteration steps per training epoch.\nlogger : logging.Logger\n    Logger instance for logging messages related to training.\ndevice : torch.device\n    Device on which computations will be performed (CPU or CUDA).</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>load_model() -> None\n    Loads and configures the Electra generator and discriminator models.\nload_optimization() -> None\n    Sets up the optimizer and learning rate scheduler based on the optimization configuration.\nprepare_data_model_optimizer() -> None\n    Prepares the dataset, models, and optimizer for training.\nreplace_masked_tokens_from_generator(masked_inputs: torch.Tensor, original_inputs: torch.Tensor, logits: torch.Tensor, special_mask_id: int, discriminator_sampling: str = \"multinomial\") -> Tuple[torch.Tensor, torch.Tensor]\n    Replaces masked tokens with tokens sampled from the generator and generates labels for discriminator training.\ntrain() -> None\n    Trains the Electra model, which includes both the generator and discriminator, and saves the results and models.</p>\n", "bases": "PretrainLM"}, {"fullname": "finlm.models.PretrainElectra.__init__", "modulename": "finlm.models", "qualname": "PretrainElectra.__init__", "kind": "function", "doc": "<p>Initializes the PretrainElectra class with the given configuration.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>config : FinLMConfig\n    Configuration object containing dataset, model, and optimization configurations.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">config</span></span>)</span>"}, {"fullname": "finlm.models.PretrainElectra.load_model", "modulename": "finlm.models", "qualname": "PretrainElectra.load_model", "kind": "function", "doc": "<p>Loads and configures the Electra generator and discriminator models.</p>\n\n<p>This method initializes the Electra generator and discriminator models using the \nconfiguration settings, including vocabulary size, embedding size, hidden size, \nand other model parameters. The models are then moved to the appropriate device (CPU or GPU).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainElectra.load_optimization", "modulename": "finlm.models", "qualname": "PretrainElectra.load_optimization", "kind": "function", "doc": "<p>Sets up the optimizer and learning rate scheduler based on the optimization configuration.</p>\n\n<p>This method identifies the trainable parameters, ensuring that the word and position embeddings \nare not duplicated. It then calculates the total number of training steps, initializes the AdamW \noptimizer, and configures a linear learning rate scheduler with warm-up steps.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainElectra.prepare_data_model_optimizer", "modulename": "finlm.models", "qualname": "PretrainElectra.prepare_data_model_optimizer", "kind": "function", "doc": "<p>Prepares the dataset, models, and optimizer for training.</p>\n\n<p>This method calls the appropriate methods to load the dataset, load the generator and \ndiscriminator models, and set up the optimizer and learning rate scheduler.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainElectra.replace_masked_tokens_from_generator", "modulename": "finlm.models", "qualname": "PretrainElectra.replace_masked_tokens_from_generator", "kind": "function", "doc": "<p>Replaces masked tokens with tokens sampled from the generator and generates labels for discriminator training.</p>\n\n<p>This method uses the generator's output logits to replace masked tokens in the input. It generates labels \nindicating whether a token has been replaced and whether the replacement matches the original token.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>masked_inputs : torch.Tensor\n    Tensor containing the masked input token IDs.\noriginal_inputs : torch.Tensor\n    Tensor containing the original input token IDs before masking.\nlogits : torch.Tensor\n    Logits output by the generator model.\nspecial_mask_id : int\n    The token ID used for masking (typically the ID for the [MASK] token).\ndiscriminator_sampling : str, optional\n    The sampling strategy for selecting replacement tokens, either \"multinomial\" or another strategy like \"aggressive\" or \"gumbel_softmax\" (default is \"gumbel_softmax\").</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Tuple[torch.Tensor, torch.Tensor]\n    A tuple containing the discriminator inputs (with replaced tokens) and the corresponding labels tensor.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">masked_inputs</span>,</span><span class=\"param\">\t<span class=\"n\">original_inputs</span>,</span><span class=\"param\">\t<span class=\"n\">logits</span>,</span><span class=\"param\">\t<span class=\"n\">special_mask_id</span>,</span><span class=\"param\">\t<span class=\"n\">discriminator_sampling</span><span class=\"o\">=</span><span class=\"s1\">&#39;gumbel_softmax&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.PretrainElectra.train", "modulename": "finlm.models", "qualname": "PretrainElectra.train", "kind": "function", "doc": "<p>Trains the Electra model, which includes both the generator and discriminator, and saves the results and models.</p>\n\n<p>This method handles the training loop, including masking input tokens, generating replacements using the generator, \ntraining the discriminator on identifying the replaced tokens, calculating losses, updating model parameters, and \nlogging training metrics. After training is complete, it saves the models, training metrics, and plots of the loss, \naccuracy, precision, and recall.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.ElectraSimpleAttention", "modulename": "finlm.models", "qualname": "ElectraSimpleAttention", "kind": "class", "doc": "<p>A single-head attention layer for use in the Electra model.</p>\n\n<p>This class implements a simple attention mechanism, where attention scores are computed \nusing a single attention head. The attention layer includes dropout and can optionally \nreturn attention probabilities.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>hidden_size : int\n    The size of the hidden layer in the attention mechanism.\nquery : nn.Linear\n    The linear layer that projects the input to the query space.\nkey : nn.Linear\n    The linear layer that projects the input to the key space.\nvalue : nn.Linear\n    The linear layer that projects the input to the value space.\ndropout : nn.Dropout\n    Dropout applied to the attention probabilities.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>forward(sequence_embeddings: torch.Tensor, return_attention: bool = True) -> Tuple[torch.Tensor, ...]\n    Performs the forward pass, calculating the attention output and optionally returning the attention probabilities.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "finlm.models.ElectraSimpleAttention.__init__", "modulename": "finlm.models", "qualname": "ElectraSimpleAttention.__init__", "kind": "function", "doc": "<p>Initializes the ElectraSimpleAttention layer with the provided configuration.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>config : ElectraConfig\n    The configuration object containing the hidden size and dropout probability.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">config</span></span>)</span>"}, {"fullname": "finlm.models.ElectraSimpleAttention.hidden_size", "modulename": "finlm.models", "qualname": "ElectraSimpleAttention.hidden_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraSimpleAttention.query", "modulename": "finlm.models", "qualname": "ElectraSimpleAttention.query", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraSimpleAttention.key", "modulename": "finlm.models", "qualname": "ElectraSimpleAttention.key", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraSimpleAttention.value", "modulename": "finlm.models", "qualname": "ElectraSimpleAttention.value", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraSimpleAttention.dropout", "modulename": "finlm.models", "qualname": "ElectraSimpleAttention.dropout", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraSimpleAttention.forward", "modulename": "finlm.models", "qualname": "ElectraSimpleAttention.forward", "kind": "function", "doc": "<p>Performs the forward pass of the attention layer.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>sequence_embeddings : torch.Tensor\n    The input sequence embeddings of shape (number of sequences over all batched documents, hidden_size).\n    Before this mechanism is applied the nested document sequences are flattened and sequence embeddings are extracted.\nreturn_attention : bool, optional\n    If True, returns the attention probabilities along with the context layer (default is True).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Tuple[torch.Tensor, ...]\n    The context layer and, optionally, the attention probabilities.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">sequence_embeddings</span>, </span><span class=\"param\"><span class=\"n\">return_attention</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.ElectraSimpleAttentionOutput", "modulename": "finlm.models", "qualname": "ElectraSimpleAttentionOutput", "kind": "class", "doc": "<p>Outputs from the ElectraSimpleAttention layer, with residual connections and aggregation.</p>\n\n<p>This class applies a dense layer, dropout, and LayerNorm to the sequence attention embeddings.\nIt also aggregates sequence embeddings by averaging and applies a residual connection.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>dense : nn.Linear\n    A linear layer applied to the attention output.\ndropout : nn.Dropout\n    Dropout applied to the attention output.\nLayerNorm : nn.LayerNorm\n    Layer normalization applied after adding the residual connection.\nout_projection : nn.Linear\n    A linear layer that projects the aggregated embeddings to the number of labels.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>forward(sequence_attention_embeddings: torch.Tensor, sequence_embeddings: torch.Tensor, original_shapes: List[int]) -> torch.Tensor\n    Performs the forward pass, applying the dense layer, residual connection, and aggregation.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "finlm.models.ElectraSimpleAttentionOutput.__init__", "modulename": "finlm.models", "qualname": "ElectraSimpleAttentionOutput.__init__", "kind": "function", "doc": "<p>Initializes the ElectraSimpleAttentionOutput layer with the provided configuration.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>config : ElectraConfig\n    The configuration object containing the hidden size, dropout probability, and number of labels.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">config</span></span>)</span>"}, {"fullname": "finlm.models.ElectraSimpleAttentionOutput.dense", "modulename": "finlm.models", "qualname": "ElectraSimpleAttentionOutput.dense", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraSimpleAttentionOutput.dropout", "modulename": "finlm.models", "qualname": "ElectraSimpleAttentionOutput.dropout", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraSimpleAttentionOutput.LayerNorm", "modulename": "finlm.models", "qualname": "ElectraSimpleAttentionOutput.LayerNorm", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraSimpleAttentionOutput.out_projection", "modulename": "finlm.models", "qualname": "ElectraSimpleAttentionOutput.out_projection", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraSimpleAttentionOutput.forward", "modulename": "finlm.models", "qualname": "ElectraSimpleAttentionOutput.forward", "kind": "function", "doc": "<p>Performs the forward pass of the attention output layer.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>sequence_attention_embeddings : torch.Tensor\n    The embeddings output from the attention layer.\nsequence_embeddings : torch.Tensor\n    The original sequence embeddings for the residual connection.\noriginal_shapes : List[int]\n    The original shapes of the sequences before flattening.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>torch.Tensor\n    The logits for each aggregated sequence.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">sequence_attention_embeddings</span>,</span><span class=\"param\">\t<span class=\"n\">sequence_embeddings</span>,</span><span class=\"param\">\t<span class=\"n\">original_shapes</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.ElectraSimpleAttentionHead", "modulename": "finlm.models", "qualname": "ElectraSimpleAttentionHead", "kind": "class", "doc": "<p>A combination of simple attention and output layers with aggregation.</p>\n\n<p>This class combines the ElectraSimpleAttention and ElectraSimpleAttentionOutput layers \nto produce a final prediction for a sequence, with optional attention probability output.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>simple_attention : ElectraSimpleAttention\n    The simple attention layer.\nattention_output : ElectraSimpleAttentionOutput\n    The output layer that processes and aggregates the attention embeddings.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>forward(sequence_embeddings: torch.Tensor, original_shapes: List[int], return_attention: bool = True) -> Tuple[torch.Tensor, ...]\n    Performs the forward pass through the attention and output layers.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "finlm.models.ElectraSimpleAttentionHead.__init__", "modulename": "finlm.models", "qualname": "ElectraSimpleAttentionHead.__init__", "kind": "function", "doc": "<p>Initializes the ElectraSimpleAttentionHead with the provided configuration.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>config : ElectraConfig\n    The configuration object containing the necessary model parameters.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">config</span></span>)</span>"}, {"fullname": "finlm.models.ElectraSimpleAttentionHead.simple_attention", "modulename": "finlm.models", "qualname": "ElectraSimpleAttentionHead.simple_attention", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraSimpleAttentionHead.attention_output", "modulename": "finlm.models", "qualname": "ElectraSimpleAttentionHead.attention_output", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraSimpleAttentionHead.forward", "modulename": "finlm.models", "qualname": "ElectraSimpleAttentionHead.forward", "kind": "function", "doc": "<p>Performs the forward pass through the attention and output layers.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>sequence_embeddings : torch.Tensor\n    The flattened input sequence embeddings of shape (number of sequences over all batched documents, hidden_size).\noriginal_shapes : List[int]\n    The original shapes of the sequences before flattening.\nreturn_attention : bool, optional\n    If True, returns the attention probabilities along with the logits (default is True).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Tuple[torch.Tensor, ...]\n    The logits for each sequence and, optionally, the attention probabilities.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">sequence_embeddings</span>, </span><span class=\"param\"><span class=\"n\">original_shapes</span>, </span><span class=\"param\"><span class=\"n\">return_attention</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.ElectraForAggregatePredictionWithAttention", "modulename": "finlm.models", "qualname": "ElectraForAggregatePredictionWithAttention", "kind": "class", "doc": "<p>An Electra model with aggregate prediction using attention mechanisms.</p>\n\n<p>This class extends the Electra model by adding a custom head for aggregate prediction.\nIt combines token embeddings into sequence embeddings, applies attention, and makes \npredictions for entire documents which consist of sequences.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>config : ElectraConfig\n    The configuration object for the Electra model.\nnum_labels : int\n    The number of labels for classification tasks.\nelectra : ElectraModel\n    The Electra encoder model for generating token embeddings.\nhead : ElectraSimpleAttentionHead\n    The custom head for making aggregate predictions with attention.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>forward(input_ids: List[List[Tensor]], attention_mask: List[List[Tensor]], labels: Optional[torch.Tensor] = None, return_attention: bool = True) -> Any\n    Performs the forward pass, generating sequence embeddings and making predictions.</p>\n", "bases": "transformers.models.electra.modeling_electra.ElectraPreTrainedModel"}, {"fullname": "finlm.models.ElectraForAggregatePredictionWithAttention.__init__", "modulename": "finlm.models", "qualname": "ElectraForAggregatePredictionWithAttention.__init__", "kind": "function", "doc": "<p>Initializes the ElectraForAggregatePredictionWithAttention model.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>config : ElectraConfig\n    The configuration object for the Electra model.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">config</span></span>)</span>"}, {"fullname": "finlm.models.ElectraForAggregatePredictionWithAttention.config", "modulename": "finlm.models", "qualname": "ElectraForAggregatePredictionWithAttention.config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraForAggregatePredictionWithAttention.num_labels", "modulename": "finlm.models", "qualname": "ElectraForAggregatePredictionWithAttention.num_labels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraForAggregatePredictionWithAttention.electra", "modulename": "finlm.models", "qualname": "ElectraForAggregatePredictionWithAttention.electra", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraForAggregatePredictionWithAttention.head", "modulename": "finlm.models", "qualname": "ElectraForAggregatePredictionWithAttention.head", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraForAggregatePredictionWithAttention.forward", "modulename": "finlm.models", "qualname": "ElectraForAggregatePredictionWithAttention.forward", "kind": "function", "doc": "<p>Performs the forward pass of the Electra model with aggregate prediction.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_ids : List[List[Tensor]]\n    A batch of input token IDs.\nattention_mask : List[List[Tensor]]\n    A batch of attention masks corresponding to the input IDs.\nlabels : Optional[torch.Tensor], optional\n    Ground truth labels for the input sequences (default is None).\nreturn_attention : bool, optional\n    If True, returns attention probabilities along with the logits (default is True).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Any\n    The loss (if labels are provided), logits, and optionally the attention probabilities.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_ids</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">attention_mask</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_attention</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.ElectraAggregationHead", "modulename": "finlm.models", "qualname": "ElectraAggregationHead", "kind": "class", "doc": "<p>Head to aggregate sequence embeddings of a batch of documents with sequences into predictions for each document.</p>\n\n<p>This class takes sequence embeddings as input and aggregates them by averaging. \nIt then passes the aggregated embeddings through a dense layer, applies dropout \nand activation, and finally projects the result to the number of output labels.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>dense : nn.Linear\n    A linear layer that densely connects all sequence embeddings.\ndropout : nn.Dropout\n    Dropout applied to the output of the dense layer.\nactivation : nn.GELU\n    Activation function applied after the dropout.\nout_projection : nn.Linear\n    A linear layer that projects the aggregated embeddings to the number of labels.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>forward(hidden_states: torch.Tensor, original_shapes: List[int]) -> torch.Tensor\n    Performs the forward pass, aggregating the sequence embeddings and generating logits.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "finlm.models.ElectraAggregationHead.__init__", "modulename": "finlm.models", "qualname": "ElectraAggregationHead.__init__", "kind": "function", "doc": "<p>Initializes the ElectraAggregationHead with the provided configuration.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>config : ElectraConfig\n    The configuration object containing the hidden size, dropout probability, \n    and number of labels.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">config</span></span>)</span>"}, {"fullname": "finlm.models.ElectraAggregationHead.dense", "modulename": "finlm.models", "qualname": "ElectraAggregationHead.dense", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraAggregationHead.dropout", "modulename": "finlm.models", "qualname": "ElectraAggregationHead.dropout", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraAggregationHead.activation", "modulename": "finlm.models", "qualname": "ElectraAggregationHead.activation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraAggregationHead.out_projection", "modulename": "finlm.models", "qualname": "ElectraAggregationHead.out_projection", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraAggregationHead.forward", "modulename": "finlm.models", "qualname": "ElectraAggregationHead.forward", "kind": "function", "doc": "<p>Performs the forward pass, aggregating the sequence embeddings and generating logits.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>hidden_states : torch.Tensor\n    The input sequence embeddings of shape (batch_size, hidden_size).\noriginal_shapes : List[int]\n    The original shapes of the sequences before flattening.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>torch.Tensor\n    The logits for each aggregated sequence.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">hidden_states</span>, </span><span class=\"param\"><span class=\"n\">original_shapes</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "finlm.models.ElectraForAggregatePrediction", "modulename": "finlm.models", "qualname": "ElectraForAggregatePrediction", "kind": "class", "doc": "<p>An Electra model with aggregate prediction for sequence embeddings.</p>\n\n<p>This class extends the Electra model by adding a custom head for aggregate prediction.\nIt takes token embeddings, aggregates sequence embeddings, and makes predictions \nfor entire sequences or documents.</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>config : ElectraConfig\n    The configuration object for the Electra model.\nnum_labels : int\n    The number of labels for classification tasks.\nelectra : ElectraModel\n    The Electra encoder model for generating token embeddings.\nhead : ElectraAggregationHead\n    The custom head for making aggregate predictions with sequence embeddings.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>forward(input_ids: List[List[Tensor]], attention_mask: List[List[Tensor]], labels: Optional[torch.Tensor] = None) -> Any\n    Performs the forward pass, generating sequence embeddings and making predictions.</p>\n", "bases": "transformers.models.electra.modeling_electra.ElectraPreTrainedModel"}, {"fullname": "finlm.models.ElectraForAggregatePrediction.__init__", "modulename": "finlm.models", "qualname": "ElectraForAggregatePrediction.__init__", "kind": "function", "doc": "<p>Initializes the ElectraForAggregatePrediction model.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>config : ElectraConfig\n    The configuration object for the Electra model.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">config</span></span>)</span>"}, {"fullname": "finlm.models.ElectraForAggregatePrediction.config", "modulename": "finlm.models", "qualname": "ElectraForAggregatePrediction.config", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraForAggregatePrediction.num_labels", "modulename": "finlm.models", "qualname": "ElectraForAggregatePrediction.num_labels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraForAggregatePrediction.electra", "modulename": "finlm.models", "qualname": "ElectraForAggregatePrediction.electra", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraForAggregatePrediction.head", "modulename": "finlm.models", "qualname": "ElectraForAggregatePrediction.head", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "finlm.models.ElectraForAggregatePrediction.forward", "modulename": "finlm.models", "qualname": "ElectraForAggregatePrediction.forward", "kind": "function", "doc": "<p>Performs the forward pass of the Electra model with aggregate prediction.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>input_ids : List[List[Tensor]]\n    A batch of input token IDs.\nattention_mask : List[List[Tensor]]\n    A batch of attention masks corresponding to the input IDs.\nlabels : Optional[torch.Tensor], optional\n    Ground truth labels for the input sequences (default is None).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>Any\n    The loss (if labels are provided) and logits.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">input_ids</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">attention_mask</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Any</span>:</span></span>", "funcdef": "def"}, {"fullname": "finlm.tokenizer", "modulename": "finlm.tokenizer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "finlm.tokenizer.FinLMTokenizer", "modulename": "finlm.tokenizer", "qualname": "FinLMTokenizer", "kind": "class", "doc": "<p>A tokenizer using the fast tokenizer wrapper class from transformers. This transformer comes with all functionalities as present for\nofficial models.</p>\n", "bases": "transformers.tokenization_utils_fast.PreTrainedTokenizerFast"}, {"fullname": "finlm.tokenizer.FinLMTokenizer.__init__", "modulename": "finlm.tokenizer", "qualname": "FinLMTokenizer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tokenizer_file</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">bos_token</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;[seq]&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">eos_token</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;[/seq]&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">unk_token</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;[unk]&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">pad_token</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;[pad]&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">mask_token</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;[mask]&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">add_prefix_space</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">trim_offsets</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span>)</span>"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();